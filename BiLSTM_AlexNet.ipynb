{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MemoEval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx4vk4QrQFKX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "61235354-d729-437d-ef7c-60aa6201b523"
      },
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv(r'train.csv')\n",
        "val_data = pd.read_csv(r'val.csv')\n",
        "test_data = pd.read_csv(r'test.csv')\n",
        "test_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image_name</th>\n",
              "      <th>Image_URL</th>\n",
              "      <th>OCR_extracted_text</th>\n",
              "      <th>corrected_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>chuck_chuck_norris_meme_10.jpg</td>\n",
              "      <td>https://gtmemes.com/wp-content/uploads/2019/03...</td>\n",
              "      <td>Some magicians can walk on water  Chuck Norris...</td>\n",
              "      <td>Some magicians can walk on water  Chuck Norris...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dr_evil_NDBB96K.png</td>\n",
              "      <td>https://i.imgur.com/NDBB96K.png</td>\n",
              "      <td>ONE MILLION DOLLARS made on imgur</td>\n",
              "      <td>ONE MILLION DOLLARS made on imgur</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>misog_2109e457d636565e2e06dce39874c5231e1.jpg</td>\n",
              "      <td>https://media0ch-a.akamaihd.net/83/96/9e457d63...</td>\n",
              "      <td>Me: Mom can my friend sleep over? Mom: That's ...</td>\n",
              "      <td>Me: Mom can my friend sleep over? Mom: That's ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>obama_2691536739_469698809820026_263513986_n.jpg</td>\n",
              "      <td>http://politicalmemes.com/wp-content/uploads/2...</td>\n",
              "      <td>THIS GUY INHERITED A MESS. DID HE WHINE ABOUT ...</td>\n",
              "      <td>THIS GUY INHERITED A MESS. DID HE WHINE ABOUT ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>kim_threat-kim-jong-un-allegedly-working-on-mu...</td>\n",
              "      <td>https://pics.me.me/threat-kim-jong-un-allegedl...</td>\n",
              "      <td>THREAT: Kim Jong Un allegedly working on multi...</td>\n",
              "      <td>THREAT: Kim Jong Un allegedly working on multi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          Image_name  ...                                     corrected_text\n",
              "0                     chuck_chuck_norris_meme_10.jpg  ...  Some magicians can walk on water  Chuck Norris...\n",
              "1                                dr_evil_NDBB96K.png  ...                  ONE MILLION DOLLARS made on imgur\n",
              "2      misog_2109e457d636565e2e06dce39874c5231e1.jpg  ...  Me: Mom can my friend sleep over? Mom: That's ...\n",
              "3   obama_2691536739_469698809820026_263513986_n.jpg  ...  THIS GUY INHERITED A MESS. DID HE WHINE ABOUT ...\n",
              "4  kim_threat-kim-jong-un-allegedly-working-on-mu...  ...  THREAT: Kim Jong Un allegedly working on multi...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpT29E57g4n7",
        "colab_type": "text"
      },
      "source": [
        "#Text - BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDHD3685x3q8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "f770d441-4c28-4ca6-9100-330ed6cb07a2"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_data = pd.read_csv(r'train.csv')\n",
        "val_data = pd.read_csv(r'val.csv')\n",
        "\n",
        "#train_data=train_data.dropna()\n",
        "#val_data=val_data.dropna()\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "val_data = val_data.reset_index(drop=True)\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "clean=[]\n",
        "for s in train_data.text_corrected:\n",
        "    s = \" \".join(re.split('[-_,.#@?()!:]',s))\n",
        "    clean.append(s.lower())\n",
        "train_data['Clean_Text'] = clean\n",
        "\n",
        "clean=[]\n",
        "for s in val_data.text_corrected:\n",
        "    s = \" \".join(re.split('[-_,.#@?()!:]',s))\n",
        "    clean.append(s.lower())\n",
        "val_data['Clean_Text'] = clean\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tk = Tokenizer(lower = True)\n",
        "tk.fit_on_texts(train_data.Clean_Text.values)\n",
        "\n",
        "X_train_seq = tk.texts_to_sequences(train_data.Clean_Text.values)\n",
        "X_val_seq = tk.texts_to_sequences(val_data.Clean_Text.values)\n",
        "X_train = pad_sequences(X_train_seq, maxlen=64, padding='post')\n",
        "X_val = pad_sequences(X_val_seq, maxlen=64, padding='post')\n",
        "\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>text_ocr</th>\n",
              "      <th>text_corrected</th>\n",
              "      <th>humour</th>\n",
              "      <th>sarcasm</th>\n",
              "      <th>offensive</th>\n",
              "      <th>motivational</th>\n",
              "      <th>overall_sentiment</th>\n",
              "      <th>Clean_Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>image_1.jpg</td>\n",
              "      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n",
              "      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n",
              "      <td>hilarious</td>\n",
              "      <td>general</td>\n",
              "      <td>not_offensive</td>\n",
              "      <td>not_motivational</td>\n",
              "      <td>very_positive</td>\n",
              "      <td>look there my friend lightyear now all sohalik...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>image_2.jpeg</td>\n",
              "      <td>The best of #10 YearChallenge! Completed in le...</td>\n",
              "      <td>The best of #10 YearChallenge! Completed in le...</td>\n",
              "      <td>not_funny</td>\n",
              "      <td>general</td>\n",
              "      <td>not_offensive</td>\n",
              "      <td>motivational</td>\n",
              "      <td>very_positive</td>\n",
              "      <td>the best of  10 yearchallenge  completed in le...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>image_3.JPG</td>\n",
              "      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n",
              "      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n",
              "      <td>very_funny</td>\n",
              "      <td>not_sarcastic</td>\n",
              "      <td>not_offensive</td>\n",
              "      <td>not_motivational</td>\n",
              "      <td>positive</td>\n",
              "      <td>sam thorne  strippin   follow follow saw every...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>image_4.png</td>\n",
              "      <td>10 Year Challenge - Sweet Dee Edition</td>\n",
              "      <td>10 Year Challenge - Sweet Dee Edition</td>\n",
              "      <td>very_funny</td>\n",
              "      <td>twisted_meaning</td>\n",
              "      <td>very_offensive</td>\n",
              "      <td>motivational</td>\n",
              "      <td>positive</td>\n",
              "      <td>10 year challenge   sweet dee edition</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>image_5.png</td>\n",
              "      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n",
              "      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n",
              "      <td>hilarious</td>\n",
              "      <td>very_twisted</td>\n",
              "      <td>very_offensive</td>\n",
              "      <td>not_motivational</td>\n",
              "      <td>neutral</td>\n",
              "      <td>10 year challenge with no filter 47 hilarious ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     image_name  ...                                         Clean_Text\n",
              "0   image_1.jpg  ...  look there my friend lightyear now all sohalik...\n",
              "1  image_2.jpeg  ...  the best of  10 yearchallenge  completed in le...\n",
              "2   image_3.JPG  ...  sam thorne  strippin   follow follow saw every...\n",
              "3   image_4.png  ...              10 year challenge   sweet dee edition\n",
              "4   image_5.png  ...  10 year challenge with no filter 47 hilarious ...\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba8zRLbVQqDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean=[]\n",
        "for s in test_data.corrected_text:\n",
        "    s = \" \".join(re.split('[-_,.#@?()!:]',s))\n",
        "    clean.append(s.lower())\n",
        "test_data['Clean_Text'] = clean\n",
        "\n",
        "X_test_seq = tk.texts_to_sequences(test_data.Clean_Text.values)\n",
        "X_test = pad_sequences(X_test_seq, maxlen=64, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV6yevxvg_77",
        "colab_type": "text"
      },
      "source": [
        "##A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4GS1_fpc1ho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent=[]\n",
        "for s in train_data.overall_sentiment:\n",
        "  if('positive' in s):\n",
        "    sent.append('positive')\n",
        "  elif('negative' in s):\n",
        "    sent.append('negative')\n",
        "  else:\n",
        "    sent.append('neutral')    \n",
        "train_data['Sentiment']=sent\n",
        "\n",
        "sent=[]\n",
        "for s in val_data.overall_sentiment:\n",
        "  if('positive' in s):\n",
        "    sent.append('positive')\n",
        "  elif('negative' in s):\n",
        "    sent.append('negative')\n",
        "  else:\n",
        "    sent.append('neutral')\n",
        "val_data['Sentiment']=sent\n",
        "\n",
        "y_train=train_data['Sentiment'].values\n",
        "y_val=val_data['Sentiment'].values\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_le = le.fit_transform(y_train)\n",
        "y_val_le = le.transform(y_val)\n",
        "y_train_cat = to_categorical(y_train_le)\n",
        "y_val_cat = to_categorical(y_val_le)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVfX3sjfv_As",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a6b9835f-ab28-411e-916f-c5625a104d85"
      },
      "source": [
        "val_data.Sentiment.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "positive    630\n",
              "neutral     307\n",
              "negative    112\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clJMMQ_wrjrS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "c2f9f2a2-6ee6-4083-aed6-a70549368ab9"
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',np.unique(train_data.Sentiment.values),\n",
        "                                                  train_data.Sentiment.values)\n",
        "class_weights= {0:class_weights[0],1:class_weights[1],2:class_weights[2]}\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GRU\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import datetime, os\n",
        "\n",
        "Batch_size = 32\n",
        "vocabulary_size = len(tk.word_counts.keys())+1\n",
        "max_words = 64\n",
        "embedding_size = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words,))\n",
        "#                    weights=[embedding_matrix], trainable=False))\n",
        "model.add(Dropout(0.5))\n",
        "#model.add(Bidirectional(LSTM(4, dropout=0.5,recurrent_dropout=0, return_sequences=True)))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(LSTM(16))) #, dropout=0.5,recurrent_dropout=0.5, kernel_regularizer=l2(0.01) \n",
        "model.add(Dropout(0.5))\n",
        "#model.add(Dense(3, activation='softmax', kernel_regularizer=l2(0.01)))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "#adam=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, decay=1e-2)\n",
        "#sgd = keras.optimizers.SGD(lr=0.01, decay=1e-2, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "early=EarlyStopping(patience=5, restore_best_weights=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=0, verbose=1)\n",
        "\n",
        "history=model.fit(X_train, y_train_cat, validation_data=(X_val, y_val_cat), batch_size=Batch_size,\n",
        "          epochs=50, \n",
        "          class_weight=class_weights, \n",
        "          callbacks=[early,reduce_lr]\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "186/186 [==============================] - 3s 15ms/step - loss: 1.0987 - acc: 0.2780 - val_loss: 1.0941 - val_acc: 0.4032 - lr: 0.0100\n",
            "Epoch 2/50\n",
            "186/186 [==============================] - 2s 11ms/step - loss: 1.0996 - acc: 0.3306 - val_loss: 1.1001 - val_acc: 0.1935 - lr: 0.0100\n",
            "Epoch 3/50\n",
            "186/186 [==============================] - ETA: 0s - loss: 1.0987 - acc: 0.3120\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "186/186 [==============================] - 2s 11ms/step - loss: 1.0987 - acc: 0.3120 - val_loss: 1.0996 - val_acc: 0.2450 - lr: 0.0100\n",
            "Epoch 4/50\n",
            "186/186 [==============================] - 2s 11ms/step - loss: 1.0987 - acc: 0.3441 - val_loss: 1.0999 - val_acc: 0.2374 - lr: 1.0000e-03\n",
            "Epoch 5/50\n",
            "185/186 [============================>.] - ETA: 0s - loss: 1.1007 - acc: 0.3309\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
            "186/186 [==============================] - 2s 11ms/step - loss: 1.0991 - acc: 0.3310 - val_loss: 1.0990 - val_acc: 0.2745 - lr: 1.0000e-03\n",
            "Epoch 6/50\n",
            "186/186 [==============================] - ETA: 0s - loss: 1.0983 - acc: 0.3394Restoring model weights from the end of the best epoch.\n",
            "186/186 [==============================] - 2s 11ms/step - loss: 1.0983 - acc: 0.3394 - val_loss: 1.0989 - val_acc: 0.2812 - lr: 1.0000e-04\n",
            "Epoch 00006: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P37qHlj1tS1m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "ccffd095-af6d-427c-ac01-a5b3da5c2f5f"
      },
      "source": [
        "y_train_prob=model.predict(X_train)\n",
        "y_val_prob=model.predict(X_val)\n",
        "\n",
        "y_train_pred = [np.argmax(x) for x in y_train_prob]\n",
        "y_val_pred = [np.argmax(x) for x in y_val_prob]\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,auc\n",
        "\n",
        "print(\"Training Accuracy : {0}%\".format(int(100*accuracy_score(y_train_le, y_train_pred))))\n",
        "print(\"Test Accuracy : {0}%\\n\".format(int(100*accuracy_score(y_val_le, y_val_pred))))\n",
        "print(classification_report(y_val_le, y_val_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy : 41%\n",
            "Test Accuracy : 40%\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.20      0.02      0.03       112\n",
            "           1       0.30      0.71      0.42       307\n",
            "           2       0.64      0.32      0.43       630\n",
            "\n",
            "    accuracy                           0.40      1049\n",
            "   macro avg       0.38      0.35      0.30      1049\n",
            "weighted avg       0.50      0.40      0.38      1049\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ygVhJsbQ3PC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_prob=model.predict(X_test)\n",
        "y_test_pred = [np.argmax(x) for x in y_test_prob]\n",
        "\n",
        "with open('A.pkl', 'wb') as f:\n",
        "  pickle.dump(y_test_pred, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZc-WOX9wXfR",
        "colab_type": "text"
      },
      "source": [
        "##B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1xiFf-ezMRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train=[]\n",
        "for i in train_data.index:\n",
        "  labs=[]\n",
        "  labs += [0] if train_data.at[i,'humour']=='not_funny' else [1]\n",
        "  labs += [0] if train_data.at[i,'sarcasm']=='not_sarcastic' else [1]\n",
        "  labs += [0] if train_data.at[i,'offensive']=='not_offensive' else [1]\n",
        "  labs += [0] if train_data.at[i,'motivational']=='not_motivational' else [1]\n",
        "  y_train.append(labs)\n",
        "\n",
        "y_val=[]\n",
        "for i in val_data.index:\n",
        "  labs=[]\n",
        "  labs += [0] if val_data.at[i,'humour']=='not_funny' else [1]\n",
        "  labs += [0] if val_data.at[i,'sarcasm']=='not_sarcastic' else [1]\n",
        "  labs += [0] if val_data.at[i,'offensive']=='not_offensive' else [1]\n",
        "  labs += [0] if val_data.at[i,'motivational']=='not_motivational' else [1]\n",
        "  y_val.append(labs)\n",
        "\n",
        "y_train=np.array(y_train)\n",
        "y_val=np.array(y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLaXixtz1ly-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "d4bb9efc-67a2-4e32-f58c-b9661b6d89ab"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GRU\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import datetime, os\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "cls = 3   #Toggle 0,1,2,3 for humor, sarcasm, offensive, motivational respectively\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train[:,cls]), y_train[:,cls])\n",
        "class_weights= {0:class_weights[0],1:class_weights[1]}\n",
        "\n",
        "Batch_size = 32\n",
        "vocabulary_size = len(tk.word_counts.keys())+1\n",
        "max_words = 64\n",
        "embedding_size = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words,))\n",
        "#                    weights=[embedding_matrix], trainable=False))\n",
        "model.add(Dropout(0.5))\n",
        "#model.add(Bidirectional(LSTM(4, dropout=0.5,recurrent_dropout=0, return_sequences=True)))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(LSTM(32)))#, dropout=0.5,recurrent_dropout=0.5, kernel_regularizer=l2(0.01))))\n",
        "model.add(Dropout(0.5))\n",
        "#model.add(Dense(3, activation='softmax', kernel_regularizer=l2(0.01)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "#adam=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, decay=1e-2)\n",
        "#sgd = keras.optimizers.SGD(lr=0.01, decay=1e-2, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "early=EarlyStopping(patience=5, restore_best_weights=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=0, verbose=1)\n",
        "\n",
        "history=model.fit(X_train, y_train[:,cls], validation_data=(X_val, y_val[:,cls]), batch_size=Batch_size,\n",
        "          epochs=50, \n",
        "          class_weight=class_weights,\n",
        "          callbacks=[early,reduce_lr]\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "186/186 [==============================] - 4s 21ms/step - loss: 0.6936 - acc: 0.5304 - val_loss: 0.6932 - val_acc: 0.5091 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "186/186 [==============================] - 2s 13ms/step - loss: 0.6868 - acc: 0.5558 - val_loss: 0.6790 - val_acc: 0.5624 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "184/186 [============================>.] - ETA: 0s - loss: 0.6215 - acc: 0.6595\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "186/186 [==============================] - 2s 13ms/step - loss: 0.6214 - acc: 0.6591 - val_loss: 0.7367 - val_acc: 0.5291 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "186/186 [==============================] - ETA: 0s - loss: 0.4651 - acc: 0.7887\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "186/186 [==============================] - 2s 13ms/step - loss: 0.4651 - acc: 0.7887 - val_loss: 0.8081 - val_acc: 0.5291 - lr: 1.0000e-04\n",
            "Epoch 5/50\n",
            "185/186 [============================>.] - ETA: 0s - loss: 0.4307 - acc: 0.7926\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "186/186 [==============================] - 2s 13ms/step - loss: 0.4300 - acc: 0.7929 - val_loss: 0.8136 - val_acc: 0.5329 - lr: 1.0000e-05\n",
            "Epoch 6/50\n",
            "184/186 [============================>.] - ETA: 0s - loss: 0.4277 - acc: 0.7991\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "186/186 [==============================] - 2s 13ms/step - loss: 0.4277 - acc: 0.7996 - val_loss: 0.8138 - val_acc: 0.5329 - lr: 1.0000e-06\n",
            "Epoch 7/50\n",
            "184/186 [============================>.] - ETA: 0s - loss: 0.4227 - acc: 0.8050Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "186/186 [==============================] - 2s 13ms/step - loss: 0.4226 - acc: 0.8048 - val_loss: 0.8138 - val_acc: 0.5329 - lr: 1.0000e-07\n",
            "Epoch 00007: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYy3z_dq7eYO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "31d823ec-a484-4166-f74b-da41cfa0d768"
      },
      "source": [
        "y_train_prob=model.predict(X_train)\n",
        "y_val_prob=model.predict(X_val)\n",
        "\n",
        "y_train_pred = [1 if x>0.5 else 0 for x in y_train_prob]\n",
        "y_val_pred = [1 if x>0.5 else 0 for x in y_val_prob]\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,auc\n",
        "\n",
        "print(\"Training Accuracy : {0}%\".format(int(100*accuracy_score(y_train[:,cls], y_train_pred))))\n",
        "print(\"Test Accuracy : {0}%\\n\".format(int(100*accuracy_score(y_val[:,cls], y_val_pred))))\n",
        "print(classification_report(y_val[:,cls], y_val_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy : 72%\n",
            "Test Accuracy : 56%\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.72      0.68       681\n",
            "           1       0.34      0.27      0.30       368\n",
            "\n",
            "    accuracy                           0.56      1049\n",
            "   macro avg       0.50      0.50      0.49      1049\n",
            "weighted avg       0.54      0.56      0.55      1049\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd8_SsagSqtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_prob=model.predict(X_test)\n",
        "y_test_pred = [1 if x>0.5 else 0 for x in y_test_prob]\n",
        "\n",
        "with open('B'+str(cls)+'.pkl', 'wb') as f:\n",
        "  pickle.dump(y_test_pred, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz68VkvJOrr_",
        "colab_type": "text"
      },
      "source": [
        "##C"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eVB5kkiLfE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train=[]\n",
        "for i in train_data.index:\n",
        "  labs=[]\n",
        "  x=1\n",
        "  if train_data.at[i,'humour']=='not_funny':\n",
        "    x=0\n",
        "  elif train_data.at[i,'humour']=='very_funny':\n",
        "    x=2\n",
        "  elif train_data.at[i,'humour']=='hilarious':\n",
        "    x=3\n",
        "  labs+=[x]\n",
        "  x=1\n",
        "  if train_data.at[i,'sarcasm']=='not_sarcastic':\n",
        "    x=0\n",
        "  elif train_data.at[i,'sarcasm']=='twisted_meaning':\n",
        "    x=2\n",
        "  elif train_data.at[i,'sarcasm']=='very_twisted':\n",
        "    x=3\n",
        "  labs+=[x]\n",
        "  x=1\n",
        "  if train_data.at[i,'offensive']=='not_offensive':\n",
        "    x=0\n",
        "  elif train_data.at[i,'offensive']=='very_offensive':\n",
        "    x=2\n",
        "  elif train_data.at[i,'offensive']=='hateful_offensive':\n",
        "    x=3\n",
        "  labs+=[x]\n",
        "  y_train.append(labs)\n",
        "\n",
        "y_val=[]\n",
        "for i in val_data.index:\n",
        "  labs=[]\n",
        "  x=1\n",
        "  if val_data.at[i,'humour']=='not_funny':\n",
        "    x=0\n",
        "  elif val_data.at[i,'humour']=='very_funny':\n",
        "    x=2\n",
        "  elif val_data.at[i,'humour']=='hilarious':\n",
        "    x=3\n",
        "  labs+=[x]\n",
        "  x=1\n",
        "  if val_data.at[i,'sarcasm']=='not_sarcastic':\n",
        "    x=0\n",
        "  elif val_data.at[i,'sarcasm']=='twisted_meaning':\n",
        "    x=2\n",
        "  elif val_data.at[i,'sarcasm']=='very_twisted':\n",
        "    x=3\n",
        "  labs+=[x]\n",
        "  x=1\n",
        "  if val_data.at[i,'offensive']=='not_offensive':\n",
        "    x=0\n",
        "  elif val_data.at[i,'offensive']=='very_offensive':\n",
        "    x=2\n",
        "  elif val_data.at[i,'offensive']=='hateful_offensive':\n",
        "    x=3\n",
        "  labs+=[x]\n",
        "  y_val.append(labs)\n",
        "\n",
        "y_train=np.array(y_train)\n",
        "y_val=np.array(y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeOLKf-TYMUL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "272a8888-152b-41db-b1c2-5d2b3a6d88f7"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GRU\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import datetime, os\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "cls = 2   #Toggle 0,1,2 for humor, sarcasm, offensive respectively\n",
        "\n",
        "class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train[:,cls]), y_train[:,cls])\n",
        "class_weights= {0:class_weights[0],1:class_weights[1],2:class_weights[2],3:class_weights[3]}\n",
        "\n",
        "Batch_size = 32\n",
        "vocabulary_size = len(tk.word_counts.keys())+1\n",
        "max_words = 64\n",
        "embedding_size = 32\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words,))\n",
        "#                    weights=[embedding_matrix], trainable=False))\n",
        "model.add(Dropout(0.5))\n",
        "#model.add(Bidirectional(LSTM(4, dropout=0.5,recurrent_dropout=0, return_sequences=True)))\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(LSTM(16)))#, dropout=0.5,recurrent_dropout=0.5, kernel_regularizer=l2(0.01))))\n",
        "model.add(Dropout(0.5))\n",
        "#model.add(Dense(3, activation='softmax', kernel_regularizer=l2(0.01)))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "#adam=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, decay=1e-2)\n",
        "#sgd = keras.optimizers.SGD(lr=0.01, decay=1e-2, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "\n",
        "history=model.fit(X_train, y_train[:,cls], validation_data=(X_val, y_val[:,cls]), batch_size=Batch_size,\n",
        "          epochs=50, \n",
        "          class_weight=class_weights,\n",
        "          callbacks=[early,reduce_lr]\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "186/186 [==============================] - 3s 17ms/step - loss: 1.3872 - acc: 0.2275 - val_loss: 1.3795 - val_acc: 0.3413 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "184/186 [============================>.] - ETA: 0s - loss: 1.3831 - acc: 0.3606\n",
            "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "186/186 [==============================] - 2s 13ms/step - loss: 1.3792 - acc: 0.3599 - val_loss: 1.3916 - val_acc: 0.2869 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "186/186 [==============================] - 2s 13ms/step - loss: 1.3492 - acc: 0.4038 - val_loss: 1.3769 - val_acc: 0.3098 - lr: 1.0000e-04\n",
            "Epoch 4/50\n",
            "184/186 [============================>.] - ETA: 0s - loss: 1.3338 - acc: 0.4378\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "186/186 [==============================] - 2s 13ms/step - loss: 1.3327 - acc: 0.4363 - val_loss: 1.3790 - val_acc: 0.2974 - lr: 1.0000e-04\n",
            "Epoch 5/50\n",
            "186/186 [==============================] - 2s 13ms/step - loss: 1.3219 - acc: 0.4636 - val_loss: 1.3691 - val_acc: 0.3213 - lr: 1.0000e-05\n",
            "Epoch 6/50\n",
            "183/186 [============================>.] - ETA: 0s - loss: 1.3171 - acc: 0.4728\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "186/186 [==============================] - 2s 13ms/step - loss: 1.3197 - acc: 0.4722 - val_loss: 1.3702 - val_acc: 0.3136 - lr: 1.0000e-05\n",
            "Epoch 7/50\n",
            "184/186 [============================>.] - ETA: 0s - loss: 1.3250 - acc: 0.4725\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "186/186 [==============================] - 2s 13ms/step - loss: 1.3220 - acc: 0.4728 - val_loss: 1.3701 - val_acc: 0.3136 - lr: 1.0000e-06\n",
            "Epoch 8/50\n",
            "184/186 [============================>.] - ETA: 0s - loss: 1.3161 - acc: 0.4557\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "186/186 [==============================] - 2s 13ms/step - loss: 1.3190 - acc: 0.4557 - val_loss: 1.3701 - val_acc: 0.3136 - lr: 1.0000e-07\n",
            "Epoch 9/50\n",
            "185/186 [============================>.] - ETA: 0s - loss: 1.3209 - acc: 0.4677\n",
            "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
            "186/186 [==============================] - 2s 13ms/step - loss: 1.3193 - acc: 0.4681 - val_loss: 1.3701 - val_acc: 0.3136 - lr: 1.0000e-08\n",
            "Epoch 10/50\n",
            "186/186 [==============================] - ETA: 0s - loss: 1.3233 - acc: 0.4573Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
            "186/186 [==============================] - 2s 13ms/step - loss: 1.3233 - acc: 0.4573 - val_loss: 1.3701 - val_acc: 0.3136 - lr: 1.0000e-09\n",
            "Epoch 00010: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKoUNIPuZNtC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "52bd18a7-67e8-426f-ed84-30aa29eee728"
      },
      "source": [
        "y_train_prob=model.predict(X_train)\n",
        "y_val_prob=model.predict(X_val)\n",
        "\n",
        "y_train_pred = [np.argmax(x) for x in y_train_prob]\n",
        "y_val_pred = [np.argmax(x) for x in y_val_prob]\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,auc\n",
        "\n",
        "print(\"Training Accuracy : {0}%\".format(int(100*accuracy_score(y_train[:,cls], y_train_pred))))\n",
        "print(\"Test Accuracy : {0}%\\n\".format(int(100*accuracy_score(y_val[:,cls], y_val_pred))))\n",
        "print(classification_report(y_val[:,cls], y_val_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy : 52%\n",
            "Test Accuracy : 32%\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.48      0.44       424\n",
            "           1       0.40      0.14      0.21       389\n",
            "           2       0.20      0.36      0.26       207\n",
            "           3       0.03      0.03      0.03        29\n",
            "\n",
            "    accuracy                           0.32      1049\n",
            "   macro avg       0.26      0.26      0.24      1049\n",
            "weighted avg       0.35      0.32      0.31      1049\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74eb_ZCbXOLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_prob=model.predict(X_test)\n",
        "y_test_pred = [np.argmax(x) for x in y_test_prob]\n",
        "\n",
        "with open('C'+str(cls)+'.pkl', 'wb') as f:\n",
        "  pickle.dump(y_test_pred, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ltqER9V1K3y",
        "colab_type": "text"
      },
      "source": [
        "#Image - AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGL46Co91MUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/My Drive/\"\n",
        "!kaggle datasets download --unzip williamscott701/memotion-dataset-7k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SNp9bnVe5V9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "from keras.preprocessing import image\n",
        "train_images=[]\n",
        "val_images=[]\n",
        "\n",
        "HEIGHT=224\n",
        "WIDTH=224\n",
        "\n",
        "for filename in val_data.image_name:\n",
        "  img = image.load_img('/content/memotion_dataset_7k/images/'+filename, target_size=(HEIGHT, WIDTH))\n",
        "  x = image.img_to_array(img)\n",
        "  val_images.append(x/255.)\n",
        "\n",
        "for filename in train_data.image_name:\n",
        "  img = image.load_img('/content/memotion_dataset_7k/images/'+filename, target_size=(HEIGHT, WIDTH))\n",
        "  x = image.img_to_array(img)\n",
        "  train_images.append(x/255.)\n",
        "\n",
        "train_images=np.array(train_images)\n",
        "val_images=np.array(val_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH3t1fM962Xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_images=[]\n",
        "for filename in test_data.Image_name:\n",
        "  img = image.load_img('/content/test/'+filename, target_size=(HEIGHT, WIDTH))\n",
        "  x = image.img_to_array(img)\n",
        "  test_images.append(x/255.)\n",
        "\n",
        "test_images=np.array(test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnJdlmm0wbFv",
        "colab_type": "text"
      },
      "source": [
        "##A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fctVmprr5hYj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "829edb9e-7931-4817-a4df-e2108163efb7"
      },
      "source": [
        "sent=[]\n",
        "for s in train_data.overall_sentiment:\n",
        "  if('positive' in s):\n",
        "    sent.append('positive')\n",
        "  elif('negative' in s):\n",
        "    sent.append('negative')\n",
        "  else:\n",
        "    sent.append('neutral')\n",
        "    \n",
        "train_data['Sentiment']=sent\n",
        "\n",
        "sent=[]\n",
        "for s in val_data.overall_sentiment:\n",
        "  if('positive' in s):\n",
        "    sent.append('positive')\n",
        "  elif('negative' in s):\n",
        "    sent.append('negative')\n",
        "  else:\n",
        "    sent.append('neutral')\n",
        "    \n",
        "val_data['Sentiment']=sent\n",
        "\n",
        "y_train=train_data['Sentiment'].values\n",
        "y_val=val_data['Sentiment'].values\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_le = le.fit_transform(y_train)\n",
        "y_val_le = le.transform(y_val)\n",
        "y_train_cat = to_categorical(y_train_le)\n",
        "y_val_cat = to_categorical(y_val_le)\n",
        "\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import BatchNormalization, ZeroPadding2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "img_shape=(224, 224, 3)\n",
        "n_classes=3\n",
        "l2_reg=0.\n",
        "dp=0.3\n",
        "\n",
        "model = Sequential()\n",
        "# Layer 1\n",
        "model.add(Conv2D(96, (11, 11), input_shape=img_shape,\n",
        "  padding='same', kernel_regularizer=l2(l2_reg)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(dp))\n",
        "\n",
        "# Layer 2\n",
        "model.add(Conv2D(128, (5, 5), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(dp))\n",
        "\n",
        "# Layer 3\n",
        "model.add(ZeroPadding2D((1, 1)))\n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(dp))\n",
        "\n",
        "# Layer 4\n",
        "model.add(ZeroPadding2D((1, 1)))\n",
        "model.add(Conv2D(256, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dp))\n",
        "\n",
        "# Layer 5\n",
        "model.add(ZeroPadding2D((1, 1)))\n",
        "model.add(Conv2D(256, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(dp))\n",
        "\n",
        "# Layer 6\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dp))\n",
        "\n",
        "# Layer 7\n",
        "model.add(Dense(1024))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dp))\n",
        "\n",
        "# Layer 8\n",
        "model.add(Dense(n_classes))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=['categorical_crossentropy'],\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',np.unique(train_data.Sentiment.values),\n",
        "                                                  train_data.Sentiment.values)\n",
        "class_weights= {0:class_weights[0],1:class_weights[1],2:class_weights[2]}\n",
        "\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 64\n",
        "STEPS_PER_EPOCH = int(len(train_data)/BATCH_SIZE)+1\n",
        "VALIDATION_STEPS = int(len(val_data)/BATCH_SIZE)+1\n",
        "\n",
        "early=EarlyStopping(patience=5, restore_best_weights=False, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, verbose=1)\n",
        "\n",
        "history = model.fit(\n",
        "    train_images, y_train_cat,\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    validation_data=(val_images, y_val_cat),\n",
        "    validation_steps=VALIDATION_STEPS,\n",
        "    callbacks=[early,reduce_lr],\n",
        "    class_weight=class_weights,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "93/93 [==============================] - 27s 290ms/step - loss: 1.1292 - accuracy: 0.3202 - val_loss: 1.1423 - val_accuracy: 0.1068 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 1.0972 - accuracy: 0.3286\n",
            "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "93/93 [==============================] - 27s 288ms/step - loss: 1.0972 - accuracy: 0.3286 - val_loss: 1.1604 - val_accuracy: 0.2555 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "93/93 [==============================] - 27s 288ms/step - loss: 1.0844 - accuracy: 0.3559 - val_loss: 1.1039 - val_accuracy: 0.2498 - lr: 1.0000e-04\n",
            "Epoch 4/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 1.0764 - accuracy: 0.3591\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "93/93 [==============================] - 27s 288ms/step - loss: 1.0764 - accuracy: 0.3591 - val_loss: 1.1072 - val_accuracy: 0.2126 - lr: 1.0000e-04\n",
            "Epoch 5/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 1.0674 - accuracy: 0.3633\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "93/93 [==============================] - 27s 288ms/step - loss: 1.0674 - accuracy: 0.3633 - val_loss: 1.1268 - val_accuracy: 0.1745 - lr: 1.0000e-05\n",
            "Epoch 6/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 1.0668 - accuracy: 0.3604\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "93/93 [==============================] - 27s 288ms/step - loss: 1.0668 - accuracy: 0.3604 - val_loss: 1.1401 - val_accuracy: 0.1659 - lr: 1.0000e-06\n",
            "Epoch 7/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 1.0675 - accuracy: 0.3626\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "93/93 [==============================] - 27s 288ms/step - loss: 1.0675 - accuracy: 0.3626 - val_loss: 1.1466 - val_accuracy: 0.1659 - lr: 1.0000e-07\n",
            "Epoch 8/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 1.0675 - accuracy: 0.3675\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
            "93/93 [==============================] - 27s 287ms/step - loss: 1.0675 - accuracy: 0.3675 - val_loss: 1.1504 - val_accuracy: 0.1706 - lr: 1.0000e-08\n",
            "Epoch 00008: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoGsc57neLOJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "178b1919-055c-428c-dc5e-8989ac9e6a60"
      },
      "source": [
        "y_train_prob=model.predict(train_images)\n",
        "y_val_prob=model.predict(val_images)\n",
        "\n",
        "y_train_pred = [np.argmax(x) for x in y_train_prob]\n",
        "y_val_pred = [np.argmax(x) for x in y_val_prob]\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,auc\n",
        "\n",
        "print(\"Training Accuracy : {0}%\".format(int(100*accuracy_score(y_train_le, y_train_pred))))\n",
        "print(\"Test Accuracy : {0}%\\n\".format(int(100*accuracy_score(y_val_le, y_val_pred))))\n",
        "print(classification_report(y_val_le, y_val_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy : 18%\n",
            "Test Accuracy : 17%\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.10      0.71      0.18       112\n",
            "           1       0.27      0.18      0.22       307\n",
            "           2       0.64      0.07      0.12       630\n",
            "\n",
            "    accuracy                           0.17      1049\n",
            "   macro avg       0.34      0.32      0.17      1049\n",
            "weighted avg       0.48      0.17      0.16      1049\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QQHRWnakRWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_prob=model.predict(test_images)\n",
        "y_test_pred = [np.argmax(x) for x in y_test_prob]\n",
        "\n",
        "with open('A.pkl', 'wb') as f:\n",
        "  pickle.dump(y_test_pred, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POCBsLrEt_Fx",
        "colab_type": "text"
      },
      "source": [
        "##B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXElvGVst5fe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train=[]\n",
        "for i in train_data.index:\n",
        "  labs=[]\n",
        "  labs += [0] if train_data.at[i,'humour']=='not_funny' else [1]\n",
        "  labs += [0] if train_data.at[i,'sarcasm']=='not_sarcastic' else [1]\n",
        "  labs += [0] if train_data.at[i,'offensive']=='not_offensive' else [1]\n",
        "  labs += [0] if train_data.at[i,'motivational']=='not_motivational' else [1]\n",
        "  y_train.append(labs)\n",
        "\n",
        "y_val=[]\n",
        "for i in val_data.index:\n",
        "  labs=[]\n",
        "  labs += [0] if val_data.at[i,'humour']=='not_funny' else [1]\n",
        "  labs += [0] if val_data.at[i,'sarcasm']=='not_sarcastic' else [1]\n",
        "  labs += [0] if val_data.at[i,'offensive']=='not_offensive' else [1]\n",
        "  labs += [0] if val_data.at[i,'motivational']=='not_motivational' else [1]\n",
        "  y_val.append(labs)\n",
        "\n",
        "y_train=np.array(y_train)\n",
        "y_val=np.array(y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esTADy9_yuAM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "97512a00-085e-4a86-f613-c2f6a6701d87"
      },
      "source": [
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import BatchNormalization, ZeroPadding2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "cls=3   #Toggle 0,1,2,3 for humor, sarcasm, offensive, motivational respectively\n",
        "\n",
        "img_shape=(224, 224, 3)\n",
        "n_classes=1\n",
        "l2_reg=0.\n",
        "dp=0.4\n",
        "\n",
        "model = Sequential()\n",
        "# Layer 1\n",
        "model.add(Conv2D(96, (11, 11), input_shape=img_shape,\n",
        "  padding='same', kernel_regularizer=l2(l2_reg)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(dp))\n",
        "\n",
        "# Layer 2\n",
        "model.add(Conv2D(128, (5, 5), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(dp))\n",
        "\n",
        "# Layer 3\n",
        "model.add(ZeroPadding2D((1, 1)))\n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(dp))\n",
        "\n",
        "# Layer 4\n",
        "model.add(ZeroPadding2D((1, 1)))\n",
        "model.add(Conv2D(256, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dp))\n",
        "\n",
        "# Layer 5\n",
        "model.add(ZeroPadding2D((1, 1)))\n",
        "model.add(Conv2D(256, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(dp))\n",
        "\n",
        "# Layer 6\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dp))\n",
        "\n",
        "# Layer 7\n",
        "model.add(Dense(1024))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dp))\n",
        "\n",
        "# Layer 8\n",
        "model.add(Dense(n_classes))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=['binary_crossentropy'],\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train[:,cls]), y_train[:,cls])\n",
        "class_weights= {0:class_weights[0],1:class_weights[1]}\n",
        "\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "EPOCHS = 50\n",
        "STEPS_PER_EPOCH = int(len(train_data)/BATCH_SIZE)+1\n",
        "VALIDATION_STEPS = int(len(val_data)/BATCH_SIZE)+1\n",
        "\n",
        "early=EarlyStopping(patience=5, restore_best_weights=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, verbose=1)\n",
        "\n",
        "history=model.fit(train_images, y_train[:,cls],\n",
        "\tvalidation_data=(val_images, y_val[:,cls]),\n",
        "\tepochs=50, batch_size=64, class_weight=class_weights, callbacks=[early, reduce_lr])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "93/93 [==============================] - 27s 289ms/step - loss: 0.7881 - accuracy: 0.4331 - val_loss: 1.1695 - val_accuracy: 0.5834 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "93/93 [==============================] - 27s 288ms/step - loss: 0.7621 - accuracy: 0.4222 - val_loss: 0.6935 - val_accuracy: 0.5882 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.7536 - accuracy: 0.4646\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "93/93 [==============================] - 27s 286ms/step - loss: 0.7536 - accuracy: 0.4646 - val_loss: 0.8805 - val_accuracy: 0.5605 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.7347 - accuracy: 0.5162\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "93/93 [==============================] - 27s 285ms/step - loss: 0.7347 - accuracy: 0.5162 - val_loss: 0.7392 - val_accuracy: 0.4728 - lr: 1.0000e-04\n",
            "Epoch 5/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.7229 - accuracy: 0.5194\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "93/93 [==============================] - 27s 285ms/step - loss: 0.7229 - accuracy: 0.5194 - val_loss: 0.7424 - val_accuracy: 0.4595 - lr: 1.0000e-05\n",
            "Epoch 6/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.7284 - accuracy: 0.5194\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "93/93 [==============================] - 27s 293ms/step - loss: 0.7284 - accuracy: 0.5194 - val_loss: 0.7379 - val_accuracy: 0.4585 - lr: 1.0000e-06\n",
            "Epoch 7/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.7226 - accuracy: 0.5284Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "93/93 [==============================] - 27s 286ms/step - loss: 0.7226 - accuracy: 0.5284 - val_loss: 0.7352 - val_accuracy: 0.4576 - lr: 1.0000e-07\n",
            "Epoch 00007: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0kCkkmdr1Tp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "2d669632-80a1-4945-9e35-d12f9984a162"
      },
      "source": [
        "y_train_prob=model.predict(train_images)\n",
        "y_val_prob=model.predict(val_images)\n",
        "\n",
        "y_train_pred = [1 if x>0.5 else 0 for x in y_train_prob]\n",
        "y_val_pred = [1 if x>0.5 else 0 for x in y_val_prob]\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,auc\n",
        "\n",
        "print(\"Training Accuracy : {0}%\".format(int(100*accuracy_score(y_train[:,cls], y_train_pred))))\n",
        "print(\"Test Accuracy : {0}%\\n\".format(int(100*accuracy_score(y_val[:,cls], y_val_pred))))\n",
        "print(classification_report(y_val[:,cls], y_val_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy : 60%\n",
            "Test Accuracy : 58%\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.79      0.71       681\n",
            "           1       0.36      0.22      0.27       368\n",
            "\n",
            "    accuracy                           0.59      1049\n",
            "   macro avg       0.50      0.50      0.49      1049\n",
            "weighted avg       0.55      0.59      0.56      1049\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH168qwEsfnQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_prob=model.predict(test_images)\n",
        "y_test_pred = [1 if x>0.5 else 0 for x in y_test_prob]\n",
        "\n",
        "with open('B'+str(cls)+'.pkl', 'wb') as f:\n",
        "  pickle.dump(y_test_pred, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCn_KOh9UH1l",
        "colab_type": "text"
      },
      "source": [
        "##C"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90tNlSoSUJGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train=[]\n",
        "for i in train_data.index:\n",
        "  labs=[]\n",
        "  x=1\n",
        "  if train_data.at[i,'humour']=='not_funny':\n",
        "    x=0\n",
        "  elif train_data.at[i,'humour']=='very_funny':\n",
        "    x=2\n",
        "  elif train_data.at[i,'humour']=='hilarious':\n",
        "    x=3\n",
        "  labs+=[x]\n",
        "  x=1\n",
        "  if train_data.at[i,'sarcasm']=='not_sarcastic':\n",
        "    x=0\n",
        "  elif train_data.at[i,'sarcasm']=='twisted_meaning':\n",
        "    x=2\n",
        "  elif train_data.at[i,'sarcasm']=='very_twisted':\n",
        "    x=3\n",
        "  labs+=[x]\n",
        "  x=1\n",
        "  if train_data.at[i,'offensive']=='not_offensive':\n",
        "    x=0\n",
        "  elif train_data.at[i,'offensive']=='very_offensive':\n",
        "    x=2\n",
        "  elif train_data.at[i,'offensive']=='hateful_offensive':\n",
        "    x=3\n",
        "  labs+=[x]\n",
        "  y_train.append(labs)\n",
        "\n",
        "y_val=[]\n",
        "for i in val_data.index:\n",
        "  labs=[]\n",
        "  x=1\n",
        "  if val_data.at[i,'humour']=='not_funny':\n",
        "    x=0\n",
        "  elif val_data.at[i,'humour']=='very_funny':\n",
        "    x=2\n",
        "  elif val_data.at[i,'humour']=='hilarious':\n",
        "    x=3\n",
        "  labs+=[x]\n",
        "  x=1\n",
        "  if val_data.at[i,'sarcasm']=='not_sarcastic':\n",
        "    x=0\n",
        "  elif val_data.at[i,'sarcasm']=='twisted_meaning':\n",
        "    x=2\n",
        "  elif val_data.at[i,'sarcasm']=='very_twisted':\n",
        "    x=3\n",
        "  labs+=[x]\n",
        "  x=1\n",
        "  if val_data.at[i,'offensive']=='not_offensive':\n",
        "    x=0\n",
        "  elif val_data.at[i,'offensive']=='very_offensive':\n",
        "    x=2\n",
        "  elif val_data.at[i,'offensive']=='hateful_offensive':\n",
        "    x=3\n",
        "  labs+=[x]\n",
        "  y_val.append(labs)\n",
        "\n",
        "y_train=np.array(y_train)\n",
        "y_val=np.array(y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL5pdXCLYbQa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "e627a1d1-f8b7-446f-dc8d-cee4dc3a5357"
      },
      "source": [
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import BatchNormalization, ZeroPadding2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "cls=2   #Toggle 0,1,2 for humor, sarcasm, offensive respectively\n",
        "\n",
        "img_shape=(224, 224, 3)\n",
        "n_classes=4\n",
        "l2_reg=0.\n",
        "dp=0.2\n",
        "\n",
        "model = Sequential()\n",
        "# Layer 1\n",
        "model.add(Conv2D(96, (11, 11), input_shape=img_shape,\n",
        "  padding='same', kernel_regularizer=l2(l2_reg)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(dp))\n",
        "\n",
        "# Layer 2\n",
        "model.add(Conv2D(128, (5, 5), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(dp))\n",
        "\n",
        "# Layer 3\n",
        "model.add(ZeroPadding2D((1, 1)))\n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(dp))\n",
        "\n",
        "# Layer 4\n",
        "model.add(ZeroPadding2D((1, 1)))\n",
        "model.add(Conv2D(256, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dp))\n",
        "\n",
        "# Layer 5\n",
        "model.add(ZeroPadding2D((1, 1)))\n",
        "model.add(Conv2D(256, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(dp))\n",
        "\n",
        "# Layer 6\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dp))\n",
        "\n",
        "# Layer 7\n",
        "model.add(Dense(1024))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(dp))\n",
        "\n",
        "# Layer 8\n",
        "model.add(Dense(n_classes))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=['sparse_categorical_crossentropy'],\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train[:,cls]), y_train[:,cls])\n",
        "class_weights= {0:class_weights[0],1:class_weights[1],2:class_weights[2],3:class_weights[3]}\n",
        "\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "EPOCHS = 10\n",
        "STEPS_PER_EPOCH = int(len(train_data)/BATCH_SIZE)+1\n",
        "VALIDATION_STEPS = int(len(val_data)/BATCH_SIZE)+1\n",
        "\n",
        "early=EarlyStopping(patience=5, restore_best_weights=False, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, verbose=1)\n",
        "\n",
        "history=model.fit(train_images, y_train[:,cls],\n",
        "\tvalidation_data=(val_images, y_val[:,cls]),\n",
        "\tepochs=50, batch_size=64, class_weight=class_weights, callbacks=[early, reduce_lr])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "93/93 [==============================] - 37s 397ms/step - loss: 1.4737 - accuracy: 0.1805 - val_loss: 1.4934 - val_accuracy: 0.1373 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "93/93 [==============================] - 37s 394ms/step - loss: 1.3827 - accuracy: 0.2522 - val_loss: 1.3843 - val_accuracy: 0.2202 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 1.3705 - accuracy: 0.2509\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "93/93 [==============================] - 37s 394ms/step - loss: 1.3705 - accuracy: 0.2509 - val_loss: 1.3856 - val_accuracy: 0.2440 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 1.3368 - accuracy: 0.2628\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "93/93 [==============================] - 37s 394ms/step - loss: 1.3368 - accuracy: 0.2628 - val_loss: 1.3946 - val_accuracy: 0.2164 - lr: 1.0000e-04\n",
            "Epoch 5/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 1.3210 - accuracy: 0.2744\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "93/93 [==============================] - 37s 394ms/step - loss: 1.3210 - accuracy: 0.2744 - val_loss: 1.4070 - val_accuracy: 0.1792 - lr: 1.0000e-05\n",
            "Epoch 6/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 1.3179 - accuracy: 0.2786\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "93/93 [==============================] - 37s 394ms/step - loss: 1.3179 - accuracy: 0.2786 - val_loss: 1.4111 - val_accuracy: 0.1821 - lr: 1.0000e-06\n",
            "Epoch 7/50\n",
            "93/93 [==============================] - ETA: 0s - loss: 1.3208 - accuracy: 0.2817\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "93/93 [==============================] - 37s 394ms/step - loss: 1.3208 - accuracy: 0.2817 - val_loss: 1.4124 - val_accuracy: 0.1773 - lr: 1.0000e-07\n",
            "Epoch 00007: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHyu8EJgz8_3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "be6960eb-2159-4d95-f4ca-3dcc03d28cda"
      },
      "source": [
        "y_train_prob=model.predict(train_images)\n",
        "y_val_prob=model.predict(val_images)\n",
        "\n",
        "y_train_pred = [np.argmax(x) for x in y_train_prob]\n",
        "y_val_pred = [np.argmax(x) for x in y_val_prob]\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,auc\n",
        "\n",
        "print(\"Training Accuracy : {0}%\".format(int(100*accuracy_score(y_train[:,cls], y_train_pred))))\n",
        "print(\"Test Accuracy : {0}%\\n\".format(int(100*accuracy_score(y_val[:,cls], y_val_pred))))\n",
        "print(classification_report(y_val[:,cls], y_val_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy : 22%\n",
            "Test Accuracy : 17%\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.13      0.20       424\n",
            "           1       0.40      0.18      0.25       389\n",
            "           2       0.20      0.22      0.21       207\n",
            "           3       0.03      0.52      0.06        29\n",
            "\n",
            "    accuracy                           0.18      1049\n",
            "   macro avg       0.26      0.26      0.18      1049\n",
            "weighted avg       0.35      0.18      0.21      1049\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-MOuL-p0F5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_prob=model.predict(test_images)\n",
        "y_test_pred = [np.argmax(x) for x in y_test_prob]\n",
        "\n",
        "with open('C'+str(cls)+'.pkl', 'wb') as f:\n",
        "  pickle.dump(y_test_pred, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC4QS36C7ZYc",
        "colab_type": "text"
      },
      "source": [
        "#Combined - BiLSTM+AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jpe7FoTL_7Xr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_data = pd.read_csv(r'train.csv')\n",
        "val_data = pd.read_csv(r'val.csv')\n",
        "\n",
        "#train_data=train_data.dropna()\n",
        "#val_data=val_data.dropna()\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "val_data = val_data.reset_index(drop=True)\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "clean=[]\n",
        "for s in train_data.text_corrected:\n",
        "    s = \" \".join(re.split('[-_,.#@?()!:]',s))\n",
        "    clean.append(s.lower())\n",
        "train_data['Clean_Text'] = clean\n",
        "\n",
        "clean=[]\n",
        "for s in val_data.text_corrected:\n",
        "    s = \" \".join(re.split('[-_,.#@?()!:]',s))\n",
        "    clean.append(s.lower())\n",
        "val_data['Clean_Text'] = clean\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tk = Tokenizer(lower = True)\n",
        "tk.fit_on_texts(train_data.Clean_Text.values)\n",
        "\n",
        "X_train_seq = tk.texts_to_sequences(train_data.Clean_Text.values)\n",
        "X_val_seq = tk.texts_to_sequences(val_data.Clean_Text.values)\n",
        "X_train = pad_sequences(X_train_seq, maxlen=64, padding='post')\n",
        "X_val = pad_sequences(X_val_seq, maxlen=64, padding='post')\n",
        "\n",
        "clean=[]\n",
        "for s in test_data.corrected_text:\n",
        "    s = \" \".join(re.split('[-_,.#@?()!:]',s))\n",
        "    clean.append(s.lower())\n",
        "test_data['Clean_Text'] = clean\n",
        "\n",
        "X_test_seq = tk.texts_to_sequences(test_data.Clean_Text.values)\n",
        "X_test = pad_sequences(X_test_seq, maxlen=64, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pficjYjAUNq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e5140393-a478-4e3b-d78a-236818c9d923"
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/My Drive/\"\n",
        "!kaggle datasets download --unzip williamscott701/memotion-dataset-7k"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading memotion-dataset-7k.zip to /content\n",
            " 98% 681M/695M [00:15<00:00, 44.8MB/s]\n",
            "100% 695M/695M [00:15<00:00, 45.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoLv68qaCJp5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "724385bf-c6b0-4cdc-857a-8a2766f9cf4d"
      },
      "source": [
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "from keras.preprocessing import image\n",
        "train_images=[]\n",
        "val_images=[]\n",
        "\n",
        "HEIGHT=224\n",
        "WIDTH=224\n",
        "\n",
        "for filename in val_data.image_name:\n",
        "  img = image.load_img('/content/memotion_dataset_7k/images/'+filename, target_size=(HEIGHT, WIDTH))\n",
        "  x = image.img_to_array(img)\n",
        "  val_images.append(x/255.)\n",
        "\n",
        "for filename in train_data.image_name:\n",
        "  img = image.load_img('/content/memotion_dataset_7k/images/'+filename, target_size=(HEIGHT, WIDTH))\n",
        "  x = image.img_to_array(img)\n",
        "  train_images.append(x/255.)\n",
        "\n",
        "train_images=np.array(train_images)\n",
        "val_images=np.array(val_images)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n",
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3iKYDCwC0GK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "35f326bf-3301-4a93-ace8-0e28c965bbf2"
      },
      "source": [
        "test_images=[]\n",
        "for filename in test_data.Image_name:\n",
        "  img = image.load_img('/content/test/'+filename, target_size=(HEIGHT, WIDTH))\n",
        "  x = image.img_to_array(img)\n",
        "  test_images.append(x/255.)\n",
        "\n",
        "test_images=np.array(test_images)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/Image.py:932: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6oNOQkLCqWa",
        "colab_type": "text"
      },
      "source": [
        "##A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AP4RGQPAO73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent=[]\n",
        "for s in train_data.overall_sentiment:\n",
        "  if('positive' in s):\n",
        "    sent.append('positive')\n",
        "  elif('negative' in s):\n",
        "    sent.append('negative')\n",
        "  else:\n",
        "    sent.append('neutral')    \n",
        "train_data['Sentiment']=sent\n",
        "\n",
        "sent=[]\n",
        "for s in val_data.overall_sentiment:\n",
        "  if('positive' in s):\n",
        "    sent.append('positive')\n",
        "  elif('negative' in s):\n",
        "    sent.append('negative')\n",
        "  else:\n",
        "    sent.append('neutral')\n",
        "val_data['Sentiment']=sent\n",
        "\n",
        "y_train=train_data['Sentiment'].values\n",
        "y_val=val_data['Sentiment'].values\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_le = le.fit_transform(y_train)\n",
        "y_val_le = le.transform(y_val)\n",
        "y_train_cat = to_categorical(y_train_le)\n",
        "y_val_cat = to_categorical(y_val_le)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy4qIo-tCj1A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "52f29020-f118-446e-fc2b-44f3bc4e33f9"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, Activation, ZeroPadding2D\n",
        "from tensorflow.keras.layers import Input,Dense,Bidirectional,Conv2D,MaxPooling2D,Flatten,concatenate,GlobalAveragePooling2D,BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',np.unique(train_data.Sentiment.values),\n",
        "                                                  train_data.Sentiment.values)\n",
        "class_weights= {0:class_weights[0],1:class_weights[1],2:class_weights[2]}\n",
        "\n",
        "vocabulary_size = len(tk.word_counts.keys())+1\n",
        "max_words = 64\n",
        "embedding_size = 32\n",
        "dp=0.4\n",
        "\n",
        "txt_input = Input(shape=(64,))\n",
        "x = Embedding(vocabulary_size, embedding_size, input_length=max_words,)(txt_input)\n",
        "x = Dropout(dp)(x)\n",
        "x = Bidirectional(LSTM(32))(x)#, dropout=0.4,recurrent_dropout=0.4, kernel_regularizer=l2(0.01)))(x)\n",
        "x = Dropout(dp)(x)\n",
        "x = Dense(64)(x)\n",
        "x = Model(inputs=txt_input, outputs=x)\n",
        "\n",
        "img_input = Input(shape=(224,224,3))\n",
        "y = Conv2D(96, (11, 11), padding='same')(img_input)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = Conv2D(128, (5, 5), padding='same')(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = ZeroPadding2D((1, 1))(y)\n",
        "y = Conv2D(128, (3, 3), padding='same')(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = ZeroPadding2D((1, 1))(y)\n",
        "y = Conv2D(256, (3, 3), padding='same')(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = ZeroPadding2D((1, 1))(y)\n",
        "y = Conv2D(256, (3, 3), padding='same')(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = Flatten()(y)\n",
        "y = Dense(512)(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = Dropout(dp)(y)\n",
        "y = Dense(64)(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "\n",
        "y = Model(inputs=img_input, outputs=y)\n",
        "\n",
        "shared_layer = Dense(64)\n",
        "x_sh = shared_layer(x.output)\n",
        "y_sh = shared_layer(y.output)\n",
        "\n",
        "combined = concatenate([x_sh,y_sh])#[x.output, y.output])\n",
        "z = Dense(128, activation=\"relu\")(combined)\n",
        "z = Dense(3, activation=\"softmax\")(z)\n",
        "\n",
        "model = Model(inputs=[x.input, y.input], outputs=z)\n",
        "#adam=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, decay=1e-2)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "#model.summary()\n",
        "early=EarlyStopping(patience=5, restore_best_weights=False, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, verbose=1)\n",
        "\n",
        "history=model.fit([X_train, train_images], y_train_cat,\n",
        "\tvalidation_data=([X_val, val_images], y_val_cat),\n",
        "\tepochs=30, batch_size=32, class_weight=class_weights,callbacks=[early,reduce_lr])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "186/186 [==============================] - 26s 139ms/step - loss: 1.1324 - acc: 0.3123 - val_loss: 0.9784 - val_acc: 0.5958 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "186/186 [==============================] - ETA: 0s - loss: 1.1122 - acc: 0.3290\n",
            "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "186/186 [==============================] - 25s 134ms/step - loss: 1.1122 - acc: 0.3290 - val_loss: 1.1181 - val_acc: 0.2641 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "186/186 [==============================] - ETA: 0s - loss: 1.0086 - acc: 0.4038\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "186/186 [==============================] - 25s 134ms/step - loss: 1.0086 - acc: 0.4038 - val_loss: 1.1143 - val_acc: 0.2946 - lr: 1.0000e-04\n",
            "Epoch 4/30\n",
            "186/186 [==============================] - ETA: 0s - loss: 0.9629 - acc: 0.4079\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "186/186 [==============================] - 25s 134ms/step - loss: 0.9629 - acc: 0.4079 - val_loss: 1.0878 - val_acc: 0.3232 - lr: 1.0000e-05\n",
            "Epoch 5/30\n",
            "186/186 [==============================] - ETA: 0s - loss: 0.9603 - acc: 0.4138\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "186/186 [==============================] - 25s 134ms/step - loss: 0.9603 - acc: 0.4138 - val_loss: 1.0858 - val_acc: 0.3213 - lr: 1.0000e-06\n",
            "Epoch 6/30\n",
            "186/186 [==============================] - ETA: 0s - loss: 0.9566 - acc: 0.4197\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "186/186 [==============================] - 25s 134ms/step - loss: 0.9566 - acc: 0.4197 - val_loss: 1.0845 - val_acc: 0.3260 - lr: 1.0000e-07\n",
            "Epoch 00006: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIsxGzt3Epdf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "0ef0edff-9933-41a8-e1e6-d757a06a7f61"
      },
      "source": [
        "y_train_prob=model.predict([X_train,train_images])\n",
        "y_val_prob=model.predict([X_val,val_images])\n",
        "\n",
        "y_train_pred = [np.argmax(x) for x in y_train_prob]\n",
        "y_val_pred = [np.argmax(x) for x in y_val_prob]\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,auc\n",
        "\n",
        "print(\"Training Accuracy : {0}%\".format(int(100*accuracy_score(y_train_le, y_train_pred))))\n",
        "print(\"Test Accuracy : {0}%\\n\".format(int(100*accuracy_score(y_val_le, y_val_pred))))\n",
        "print(classification_report(y_val_le, y_val_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy : 48%\n",
            "Test Accuracy : 32%\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.09      0.23      0.13       112\n",
            "           1       0.29      0.39      0.34       307\n",
            "           2       0.57      0.31      0.40       630\n",
            "\n",
            "    accuracy                           0.33      1049\n",
            "   macro avg       0.32      0.31      0.29      1049\n",
            "weighted avg       0.44      0.33      0.35      1049\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSi2UsnCGdO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_prob=model.predict([X_test,test_images])\n",
        "y_test_pred = [np.argmax(x) for x in y_test_prob]\n",
        "\n",
        "with open('A.pkl', 'wb') as f:\n",
        "  pickle.dump(y_test_pred, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOCEzgdKClhl",
        "colab_type": "text"
      },
      "source": [
        "##B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJYdVGetAdsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train=[]\n",
        "for i in train_data.index:\n",
        "  labs=[]\n",
        "  labs += [0] if train_data.at[i,'humour']=='not_funny' else [1]\n",
        "  labs += [0] if train_data.at[i,'sarcasm']=='not_sarcastic' else [1]\n",
        "  labs += [0] if train_data.at[i,'offensive']=='not_offensive' else [1]\n",
        "  labs += [0] if train_data.at[i,'motivational']=='not_motivational' else [1]\n",
        "  y_train.append(labs)\n",
        "\n",
        "y_val=[]\n",
        "for i in val_data.index:\n",
        "  labs=[]\n",
        "  labs += [0] if val_data.at[i,'humour']=='not_funny' else [1]\n",
        "  labs += [0] if val_data.at[i,'sarcasm']=='not_sarcastic' else [1]\n",
        "  labs += [0] if val_data.at[i,'offensive']=='not_offensive' else [1]\n",
        "  labs += [0] if val_data.at[i,'motivational']=='not_motivational' else [1]\n",
        "  y_val.append(labs)\n",
        "\n",
        "y_train=np.array(y_train)\n",
        "y_val=np.array(y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDcThbTHRxPC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "9f49064c-d039-43e9-aa16-3dff8d22fa9f"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, Activation, ZeroPadding2D\n",
        "from tensorflow.keras.layers import Input,Dense,Bidirectional,Conv2D,MaxPooling2D,Flatten,concatenate,GlobalAveragePooling2D,BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "cls=3   #Toggle 0,1,2,3 for humor, sarcasm, offensive, motivational respectively\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train[:,cls]), y_train[:,cls])\n",
        "class_weights= {0:class_weights[0],1:class_weights[1]}\n",
        "\n",
        "vocabulary_size = len(tk.word_counts.keys())+1\n",
        "max_words = 64\n",
        "embedding_size = 32\n",
        "dp=0.4\n",
        "\n",
        "txt_input = Input(shape=(64,))\n",
        "x = Embedding(vocabulary_size, embedding_size, input_length=max_words,)(txt_input)\n",
        "x = Dropout(dp)(x)\n",
        "x = Bidirectional(LSTM(32))(x)#, dropout=0.4,recurrent_dropout=0.4, kernel_regularizer=l2(0.01)))(x)\n",
        "x = Dropout(dp)(x)\n",
        "x = Dense(64)(x)\n",
        "x = Model(inputs=txt_input, outputs=x)\n",
        "\n",
        "img_input = Input(shape=(224,224,3))\n",
        "y = Conv2D(96, (11, 11), padding='same')(img_input)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = Conv2D(128, (5, 5), padding='same')(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = ZeroPadding2D((1, 1))(y)\n",
        "y = Conv2D(128, (3, 3), padding='same')(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = ZeroPadding2D((1, 1))(y)\n",
        "y = Conv2D(256, (3, 3), padding='same')(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = ZeroPadding2D((1, 1))(y)\n",
        "y = Conv2D(256, (3, 3), padding='same')(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = Flatten()(y)\n",
        "y = Dense(512)(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = Dropout(dp)(y)\n",
        "y = Dense(64)(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "\n",
        "y = Model(inputs=img_input, outputs=y)\n",
        "\n",
        "combined = concatenate([x.output, y.output])\n",
        "z = Dense(128, activation=\"relu\")(combined)\n",
        "z = Dense(2, activation=\"softmax\")(z)\n",
        "\n",
        "model = Model(inputs=[x.input, y.input], outputs=z)\n",
        "#adam=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, decay=1e-3)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "#model.summary()\n",
        "early=EarlyStopping(patience = 5, restore_best_weights=False, verbose=1)\n",
        "\n",
        "history=model.fit([X_train, train_images], y_train[:,cls],\n",
        "\tvalidation_data=([X_val, val_images], y_val[:,cls]),\n",
        "\tepochs=20, batch_size=32, class_weight=class_weights, callbacks=[early,reduce_lr])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "186/186 [==============================] - 26s 137ms/step - loss: 0.7084 - acc: 0.5112 - val_loss: 0.6896 - val_acc: 0.5119 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "186/186 [==============================] - ETA: 0s - loss: 0.6949 - acc: 0.5496\n",
            "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "186/186 [==============================] - 25s 133ms/step - loss: 0.6949 - acc: 0.5496 - val_loss: 0.7828 - val_acc: 0.3794 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "186/186 [==============================] - ETA: 0s - loss: 0.5672 - acc: 0.7089\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "186/186 [==============================] - 25s 133ms/step - loss: 0.5672 - acc: 0.7089 - val_loss: 0.7473 - val_acc: 0.5586 - lr: 1.0000e-04\n",
            "Epoch 4/20\n",
            "186/186 [==============================] - ETA: 0s - loss: 0.5043 - acc: 0.7594\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "186/186 [==============================] - 25s 133ms/step - loss: 0.5043 - acc: 0.7594 - val_loss: 0.7549 - val_acc: 0.5605 - lr: 1.0000e-05\n",
            "Epoch 5/20\n",
            "186/186 [==============================] - ETA: 0s - loss: 0.4979 - acc: 0.7617\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "186/186 [==============================] - 25s 133ms/step - loss: 0.4979 - acc: 0.7617 - val_loss: 0.7549 - val_acc: 0.5624 - lr: 1.0000e-06\n",
            "Epoch 6/20\n",
            "186/186 [==============================] - ETA: 0s - loss: 0.4965 - acc: 0.7648\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "186/186 [==============================] - 25s 133ms/step - loss: 0.4965 - acc: 0.7648 - val_loss: 0.7550 - val_acc: 0.5624 - lr: 1.0000e-07\n",
            "Epoch 00006: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fLkvASanNMN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e8a1d3bb-883b-4163-fd6d-e402b8ef6163"
      },
      "source": [
        "y_train_prob=model.predict([X_train,train_images])\n",
        "y_val_prob=model.predict([X_val,val_images])\n",
        "\n",
        "y_train_pred = [np.argmax(x) for x in y_train_prob]\n",
        "y_val_pred = [np.argmax(x) for x in y_val_prob]\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,auc\n",
        "\n",
        "print(\"Training Accuracy : {0}%\".format(int(100*accuracy_score(y_train[:,cls], y_train_pred))))\n",
        "print(\"Test Accuracy : {0}%\\n\".format(int(100*accuracy_score(y_val[:,cls], y_val_pred))))\n",
        "print(classification_report(y_val[:,cls], y_val_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy : 78%\n",
            "Test Accuracy : 56%\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.66      0.66       681\n",
            "           1       0.38      0.38      0.38       368\n",
            "\n",
            "    accuracy                           0.56      1049\n",
            "   macro avg       0.52      0.52      0.52      1049\n",
            "weighted avg       0.56      0.56      0.56      1049\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbWgZruCPT-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_prob=model.predict([X_test,test_images])\n",
        "y_test_pred = [np.argmax(x) for x in y_test_prob]\n",
        "\n",
        "with open('B'+str(cls)+'.pkl', 'wb') as f:\n",
        "  pickle.dump(y_test_pred, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giG-AbphVW9n",
        "colab_type": "text"
      },
      "source": [
        "##C"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNKRONbwAg2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train=[]\n",
        "for i in train_data.index:\n",
        "  labs=[]\n",
        "  x=1\n",
        "  if train_data.at[i,'humour']=='not_funny':\n",
        "    x=0\n",
        "  elif train_data.at[i,'humour']=='very_funny':\n",
        "    x=2\n",
        "  elif train_data.at[i,'humour']=='hilarious':\n",
        "    x=3\n",
        "  labs+=[x]\n",
        "  x=1\n",
        "  if train_data.at[i,'sarcasm']=='not_sarcastic':\n",
        "    x=0\n",
        "  elif train_data.at[i,'sarcasm']=='twisted_meaning':\n",
        "    x=2\n",
        "  elif train_data.at[i,'sarcasm']=='very_twisted':\n",
        "    x=3\n",
        "  labs+=[x]\n",
        "  x=1\n",
        "  if train_data.at[i,'offensive']=='not_offensive':\n",
        "    x=0\n",
        "  elif train_data.at[i,'offensive']=='very_offensive':\n",
        "    x=2\n",
        "  elif train_data.at[i,'offensive']=='hateful_offensive':\n",
        "    x=3\n",
        "  labs+=[x]\n",
        "  y_train.append(labs)\n",
        "\n",
        "y_val=[]\n",
        "for i in val_data.index:\n",
        "  labs=[]\n",
        "  x=1\n",
        "  if val_data.at[i,'humour']=='not_funny':\n",
        "    x=0\n",
        "  elif val_data.at[i,'humour']=='very_funny':\n",
        "    x=2\n",
        "  elif val_data.at[i,'humour']=='hilarious':\n",
        "    x=3\n",
        "  labs+=[x]\n",
        "  x=1\n",
        "  if val_data.at[i,'sarcasm']=='not_sarcastic':\n",
        "    x=0\n",
        "  elif val_data.at[i,'sarcasm']=='twisted_meaning':\n",
        "    x=2\n",
        "  elif val_data.at[i,'sarcasm']=='very_twisted':\n",
        "    x=3\n",
        "  labs+=[x]\n",
        "  x=1\n",
        "  if val_data.at[i,'offensive']=='not_offensive':\n",
        "    x=0\n",
        "  elif val_data.at[i,'offensive']=='very_offensive':\n",
        "    x=2\n",
        "  elif val_data.at[i,'offensive']=='hateful_offensive':\n",
        "    x=3\n",
        "  labs+=[x]\n",
        "  y_val.append(labs)\n",
        "\n",
        "y_train=np.array(y_train)\n",
        "y_val=np.array(y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhq3NGfNVZID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "0b0c6f82-9212-412f-f321-d36bd8757c4f"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, Activation, ZeroPadding2D\n",
        "from tensorflow.keras.layers import Input,Dense,Bidirectional,Conv2D,MaxPooling2D,Flatten,concatenate,GlobalAveragePooling2D,BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "cls=2   #Toggle 0,1,2 for humor, sarcasm, offensive respectively\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train[:,cls]), y_train[:,cls])\n",
        "class_weights= {0:class_weights[0],1:class_weights[1],2:class_weights[2],3:class_weights[3]}\n",
        "\n",
        "vocabulary_size = len(tk.word_counts.keys())+1\n",
        "max_words = 64\n",
        "embedding_size = 32\n",
        "dp=0.4\n",
        "\n",
        "txt_input = Input(shape=(64,))\n",
        "x = Embedding(vocabulary_size, embedding_size, input_length=max_words,)(txt_input)\n",
        "x = Dropout(dp)(x)\n",
        "x = Bidirectional(LSTM(32))(x)#, dropout=0.4,recurrent_dropout=0.4, kernel_regularizer=l2(0.01)))(x)\n",
        "x = Dropout(dp)(x)\n",
        "x = Dense(64)(x)\n",
        "x = Model(inputs=txt_input, outputs=x)\n",
        "\n",
        "img_input = Input(shape=(224,224,3))\n",
        "y = Conv2D(96, (11, 11), padding='same')(img_input)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = Conv2D(128, (5, 5), padding='same')(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = ZeroPadding2D((1, 1))(y)\n",
        "y = Conv2D(128, (3, 3), padding='same')(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = ZeroPadding2D((1, 1))(y)\n",
        "y = Conv2D(256, (3, 3), padding='same')(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = ZeroPadding2D((1, 1))(y)\n",
        "y = Conv2D(256, (3, 3), padding='same')(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = Flatten()(y)\n",
        "y = Dense(512)(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = Dropout(dp)(y)\n",
        "y = Dense(64)(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "\n",
        "y = Model(inputs=img_input, outputs=y)\n",
        "\n",
        "combined = concatenate([x.output, y.output])\n",
        "z = Dense(128, activation=\"relu\")(combined)\n",
        "z = Dense(4, activation=\"softmax\")(z)\n",
        "\n",
        "model = Model(inputs=[x.input, y.input], outputs=z)\n",
        "#adam=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, decay=1e-3)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "#model.summary()\n",
        "early=EarlyStopping(patience = 5, restore_best_weights=False, verbose=1)\n",
        "\n",
        "history=model.fit([X_train, train_images], y_train[:,cls],\n",
        "\tvalidation_data=([X_val, val_images], y_val[:,cls]),\n",
        "\tepochs=20, batch_size=32, class_weight=class_weights, callbacks=[early, reduce_lr])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "186/186 [==============================] - 25s 137ms/step - loss: 1.4209 - acc: 0.2399 - val_loss: 1.4556 - val_acc: 0.1001 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "186/186 [==============================] - 25s 133ms/step - loss: 1.4005 - acc: 0.2405 - val_loss: 1.4034 - val_acc: 0.1745 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "186/186 [==============================] - 25s 132ms/step - loss: 1.3542 - acc: 0.2810 - val_loss: 1.3441 - val_acc: 0.2927 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "186/186 [==============================] - ETA: 0s - loss: 1.0984 - acc: 0.4488\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "186/186 [==============================] - 25s 132ms/step - loss: 1.0984 - acc: 0.4488 - val_loss: 1.3572 - val_acc: 0.3031 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "186/186 [==============================] - ETA: 0s - loss: 0.7800 - acc: 0.6298\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "186/186 [==============================] - 25s 133ms/step - loss: 0.7800 - acc: 0.6298 - val_loss: 1.4845 - val_acc: 0.2993 - lr: 1.0000e-04\n",
            "Epoch 6/20\n",
            "186/186 [==============================] - ETA: 0s - loss: 0.7125 - acc: 0.6532\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "186/186 [==============================] - 25s 133ms/step - loss: 0.7125 - acc: 0.6532 - val_loss: 1.4956 - val_acc: 0.3012 - lr: 1.0000e-05\n",
            "Epoch 7/20\n",
            "186/186 [==============================] - ETA: 0s - loss: 0.7134 - acc: 0.6529\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "186/186 [==============================] - 25s 133ms/step - loss: 0.7134 - acc: 0.6529 - val_loss: 1.4964 - val_acc: 0.3031 - lr: 1.0000e-06\n",
            "Epoch 8/20\n",
            "186/186 [==============================] - ETA: 0s - loss: 0.7146 - acc: 0.6593\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "186/186 [==============================] - 25s 133ms/step - loss: 0.7146 - acc: 0.6593 - val_loss: 1.4965 - val_acc: 0.3031 - lr: 1.0000e-07\n",
            "Epoch 00008: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0x8GM6uWKIs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "fefb8260-c307-44f7-a0ae-85d66ae47995"
      },
      "source": [
        "y_train_prob=model.predict([X_train,train_images])\n",
        "y_val_prob=model.predict([X_val,val_images])\n",
        "\n",
        "y_train_pred = [np.argmax(x) for x in y_train_prob]\n",
        "y_val_pred = [np.argmax(x) for x in y_val_prob]\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,auc\n",
        "\n",
        "print(\"Training Accuracy : {0}%\".format(int(100*accuracy_score(y_train[:,cls], y_train_pred))))\n",
        "print(\"Test Accuracy : {0}%\\n\".format(int(100*accuracy_score(y_val[:,cls], y_val_pred))))\n",
        "print(classification_report(y_val[:,cls], y_val_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy : 72%\n",
            "Test Accuracy : 30%\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.33      0.35       424\n",
            "           1       0.35      0.33      0.34       389\n",
            "           2       0.21      0.24      0.23       207\n",
            "           3       0.03      0.07      0.04        29\n",
            "\n",
            "    accuracy                           0.30      1049\n",
            "   macro avg       0.24      0.24      0.24      1049\n",
            "weighted avg       0.32      0.30      0.31      1049\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87B30cQeYP0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_prob=model.predict([X_test,test_images])\n",
        "y_test_pred = [np.argmax(x) for x in y_test_prob]\n",
        "\n",
        "with open('C'+str(cls)+'.pkl', 'wb') as f:\n",
        "  pickle.dump(y_test_pred, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xbVJ_u9dz-9",
        "colab_type": "text"
      },
      "source": [
        "#Multitask - BiLSTM+AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsWJ_x24Bq2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_data = pd.read_csv(r'train.csv')\n",
        "val_data = pd.read_csv(r'val.csv')\n",
        "\n",
        "#train_data=train_data.dropna()\n",
        "#val_data=val_data.dropna()\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "val_data = val_data.reset_index(drop=True)\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "clean=[]\n",
        "for s in train_data.text_corrected:\n",
        "    s = \" \".join(re.split('[-_,.#@?()!:]',s))\n",
        "    clean.append(s.lower())\n",
        "train_data['Clean_Text'] = clean\n",
        "\n",
        "clean=[]\n",
        "for s in val_data.text_corrected:\n",
        "    s = \" \".join(re.split('[-_,.#@?()!:]',s))\n",
        "    clean.append(s.lower())\n",
        "val_data['Clean_Text'] = clean\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tk = Tokenizer(lower = True)\n",
        "tk.fit_on_texts(train_data.Clean_Text.values)\n",
        "\n",
        "X_train_seq = tk.texts_to_sequences(train_data.Clean_Text.values)\n",
        "X_val_seq = tk.texts_to_sequences(val_data.Clean_Text.values)\n",
        "X_train = pad_sequences(X_train_seq, maxlen=64, padding='post')\n",
        "X_val = pad_sequences(X_val_seq, maxlen=64, padding='post')\n",
        "\n",
        "clean=[]\n",
        "for s in test_data.corrected_text:\n",
        "    s = \" \".join(re.split('[-_,.#@?()!:]',s))\n",
        "    clean.append(s.lower())\n",
        "test_data['Clean_Text'] = clean\n",
        "\n",
        "X_test_seq = tk.texts_to_sequences(test_data.Clean_Text.values)\n",
        "X_test = pad_sequences(X_test_seq, maxlen=64, padding='post')\n",
        "\n",
        "test_images=[]\n",
        "for filename in test_data.Image_name:\n",
        "  img = image.load_img('/content/test/'+filename, target_size=(HEIGHT, WIDTH))\n",
        "  x = image.img_to_array(img)\n",
        "  test_images.append(x/255.)\n",
        "\n",
        "test_images=np.array(test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv-7fZEWDrjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/My Drive/\"\n",
        "!kaggle datasets download --unzip williamscott701/memotion-dataset-7k\n",
        "\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "from keras.preprocessing import image\n",
        "train_images=[]\n",
        "val_images=[]\n",
        "\n",
        "HEIGHT=224\n",
        "WIDTH=224\n",
        "\n",
        "for filename in val_data.image_name:\n",
        "  img = image.load_img('/content/memotion_dataset_7k/images/'+filename, target_size=(HEIGHT, WIDTH))\n",
        "  x = image.img_to_array(img)\n",
        "  val_images.append(x/255.)\n",
        "\n",
        "for filename in train_data.image_name:\n",
        "  img = image.load_img('/content/memotion_dataset_7k/images/'+filename, target_size=(HEIGHT, WIDTH))\n",
        "  x = image.img_to_array(img)\n",
        "  train_images.append(x/255.)\n",
        "\n",
        "train_images=np.array(train_images)\n",
        "val_images=np.array(val_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK_7AZYED1rZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent=[]\n",
        "for s in train_data.overall_sentiment:\n",
        "  if('positive' in s):\n",
        "    sent.append('positive')\n",
        "  elif('negative' in s):\n",
        "    sent.append('negative')\n",
        "  else:\n",
        "    sent.append('neutral')    \n",
        "train_data['Sentiment']=sent\n",
        "\n",
        "sent=[]\n",
        "for s in val_data.overall_sentiment:\n",
        "  if('positive' in s):\n",
        "    sent.append('positive')\n",
        "  elif('negative' in s):\n",
        "    sent.append('negative')\n",
        "  else:\n",
        "    sent.append('neutral')\n",
        "val_data['Sentiment']=sent\n",
        "\n",
        "y_train=train_data['Sentiment'].values\n",
        "y_val=val_data['Sentiment'].values\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_le = le.fit_transform(y_train)\n",
        "y_val_le = le.transform(y_val)\n",
        "y_train_cat = to_categorical(y_train_le)\n",
        "y_val_cat = to_categorical(y_val_le)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TEEcxdUgW_T",
        "colab_type": "text"
      },
      "source": [
        "##A+B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgC18fTuD8sM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train=[]\n",
        "for i in train_data.index:\n",
        "  labs=[]\n",
        "  labs += [0] if train_data.at[i,'humour']=='not_funny' else [1]\n",
        "  labs += [0] if train_data.at[i,'sarcasm']=='not_sarcastic' else [1]\n",
        "  labs += [0] if train_data.at[i,'offensive']=='not_offensive' else [1]\n",
        "  labs += [0] if train_data.at[i,'motivational']=='not_motivational' else [1]\n",
        "  y_train.append(labs)\n",
        "\n",
        "y_val=[]\n",
        "for i in val_data.index:\n",
        "  labs=[]\n",
        "  labs += [0] if val_data.at[i,'humour']=='not_funny' else [1]\n",
        "  labs += [0] if val_data.at[i,'sarcasm']=='not_sarcastic' else [1]\n",
        "  labs += [0] if val_data.at[i,'offensive']=='not_offensive' else [1]\n",
        "  labs += [0] if val_data.at[i,'motivational']=='not_motivational' else [1]\n",
        "  y_val.append(labs)\n",
        "\n",
        "y_train=np.array(y_train)\n",
        "y_val=np.array(y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAhL-1Q3d8d-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "f1ae664a-6593-49d6-8294-73ec3f6faefe"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, Activation, ZeroPadding2D\n",
        "from tensorflow.keras.layers import Input,Dense,Bidirectional,Conv2D,MaxPooling2D,Flatten,concatenate,GlobalAveragePooling2D,BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "cw=[]\n",
        "#class_weights = class_weight.compute_class_weight('balanced',np.unique(train_data.Sentiment.values),\n",
        "#                                                  train_data.Sentiment.values)\n",
        "#class_weights= {0:class_weights[0],1:class_weights[1],2:class_weights[2]}\n",
        "#cw.append(class_weights)\n",
        "for cls in (0,1,2,3):\n",
        "  class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train[:,cls]), y_train[:,cls])\n",
        "  class_weights= {0:class_weights[0],1:class_weights[1]}\n",
        "  cw.append(class_weights)\n",
        "\n",
        "vocabulary_size = len(tk.word_counts.keys())+1\n",
        "max_words = 64\n",
        "embedding_size = 32\n",
        "dp=0.0\n",
        "\n",
        "txt_input = Input(shape=(64,))\n",
        "x = Embedding(vocabulary_size, embedding_size, input_length=max_words,)(txt_input)\n",
        "x = Dropout(dp)(x)\n",
        "x = Bidirectional(LSTM(32, recurrent_dropout=dp, kernel_regularizer=l2(0.01), return_sequences=True))(x)\n",
        "x = Dropout(dp)(x)\n",
        "x = Bidirectional(LSTM(32, recurrent_dropout=dp, kernel_regularizer=l2(0.01)))(x)\n",
        "x = Dropout(dp)(x)\n",
        "x = Dense(128, activation=\"relu\")(x)\n",
        "x = Model(inputs=txt_input, outputs=x)\n",
        "\n",
        "img_input = Input(shape=(224,224,3))\n",
        "y = Conv2D(96, (11, 11), padding='same')(img_input)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = Conv2D(128, (5, 5), padding='same')(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = ZeroPadding2D((1, 1))(y)\n",
        "y = Conv2D(128, (3, 3), padding='same')(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = ZeroPadding2D((1, 1))(y)\n",
        "y = Conv2D(256, (3, 3), padding='same')(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = ZeroPadding2D((1, 1))(y)\n",
        "y = Conv2D(256, (3, 3), padding='same')(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = Flatten()(y)\n",
        "y = Dense(512)(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = Dropout(dp)(y)\n",
        "y = Dense(128)(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "\n",
        "y = Model(inputs=img_input, outputs=y)\n",
        "\n",
        "shared_layer = Dense(256, activation=\"relu\")\n",
        "x_sh = shared_layer(x.output)\n",
        "y_sh = shared_layer(y.output)\n",
        "\n",
        "combined = concatenate([x_sh, y_sh])#x.output, y.output])\n",
        "z = Dense(256, activation=\"relu\")(combined)\n",
        "\n",
        "#task_a = Dense(128, activation=\"relu\")(z)\n",
        "#task_a = Dense(3, activation=\"softmax\")(task_a)\n",
        "task_b0 = Dense(128, activation=\"relu\")(z)\n",
        "task_b0 = Dense(2, activation=\"softmax\")(task_b0)\n",
        "task_b1 = Dense(128, activation=\"relu\")(z)\n",
        "task_b1 = Dense(2, activation=\"softmax\")(task_b1)\n",
        "task_b2 = Dense(128, activation=\"relu\")(z)\n",
        "task_b2 = Dense(2, activation=\"softmax\")(task_b2)\n",
        "task_b3 = Dense(128, activation=\"relu\")(z)\n",
        "task_b3 = Dense(2, activation=\"softmax\")(task_b3)\n",
        "\n",
        "model = Model(inputs=[x.input, y.input], outputs=[task_b0,task_b1,task_b2,task_b3])\n",
        "#adam=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, decay=1e-3)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'], loss_weights=[1,1,1,1])\n",
        "\n",
        "#model.load_weights('model.h5')\n",
        "early=EarlyStopping(patience=5, restore_best_weights=True, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, verbose=1)\n",
        "\n",
        "history=model.fit([X_train, train_images], [y_train[:,0],y_train[:,1],y_train[:,2],y_train[:,3]],\n",
        "\tvalidation_data=([X_val, val_images], [y_val[:,0],y_val[:,1],y_val[:,2],y_val[:,3]]),\n",
        "\tepochs=20, batch_size=32, class_weight=cw, \n",
        "  callbacks=[early,reduce_lr]\n",
        "  )\n",
        "\n",
        "#model.save_weights('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 5943 samples, validate on 1049 samples\n",
            "Epoch 1/20\n",
            "5943/5943 [==============================] - 88s 15ms/sample - loss: 3.5755 - dense_6_loss: 0.6996 - dense_8_loss: 0.6989 - dense_10_loss: 0.6992 - dense_12_loss: 0.7011 - dense_6_acc: 0.4774 - dense_8_acc: 0.4831 - dense_10_acc: 0.4891 - dense_12_acc: 0.4947 - val_loss: 2.7734 - val_dense_6_loss: 0.6726 - val_dense_8_loss: 0.6562 - val_dense_10_loss: 0.6875 - val_dense_12_loss: 0.6972 - val_dense_6_acc: 0.6006 - val_dense_8_acc: 0.7407 - val_dense_10_acc: 0.5691 - val_dense_12_acc: 0.5091\n",
            "Epoch 2/20\n",
            "5920/5943 [============================>.] - ETA: 0s - loss: 2.7935 - dense_6_loss: 0.6952 - dense_8_loss: 0.6959 - dense_10_loss: 0.6947 - dense_12_loss: 0.6951 - dense_6_acc: 0.4801 - dense_8_acc: 0.5299 - dense_10_acc: 0.5154 - dense_12_acc: 0.4840\n",
            "Epoch 00002: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "5943/5943 [==============================] - 82s 14ms/sample - loss: 2.7930 - dense_6_loss: 0.6951 - dense_8_loss: 0.6954 - dense_10_loss: 0.6948 - dense_12_loss: 0.6950 - dense_6_acc: 0.4809 - dense_8_acc: 0.5289 - dense_10_acc: 0.5154 - dense_12_acc: 0.4838 - val_loss: 2.7986 - val_dense_6_loss: 0.6935 - val_dense_8_loss: 0.7154 - val_dense_10_loss: 0.7038 - val_dense_12_loss: 0.6854 - val_dense_6_acc: 0.5996 - val_dense_8_acc: 0.3241 - val_dense_10_acc: 0.4175 - val_dense_12_acc: 0.6053\n",
            "Epoch 3/20\n",
            "5920/5943 [============================>.] - ETA: 0s - loss: 2.7702 - dense_6_loss: 0.6916 - dense_8_loss: 0.6928 - dense_10_loss: 0.6927 - dense_12_loss: 0.6929 - dense_6_acc: 0.6514 - dense_8_acc: 0.3385 - dense_10_acc: 0.4400 - dense_12_acc: 0.5497\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "5943/5943 [==============================] - 81s 14ms/sample - loss: 2.7701 - dense_6_loss: 0.6915 - dense_8_loss: 0.6927 - dense_10_loss: 0.6927 - dense_12_loss: 0.6929 - dense_6_acc: 0.6517 - dense_8_acc: 0.3396 - dense_10_acc: 0.4397 - dense_12_acc: 0.5497 - val_loss: 2.7751 - val_dense_6_loss: 0.6900 - val_dense_8_loss: 0.6987 - val_dense_10_loss: 0.6965 - val_dense_12_loss: 0.6897 - val_dense_6_acc: 0.6358 - val_dense_8_acc: 0.4500 - val_dense_10_acc: 0.4604 - val_dense_12_acc: 0.5653\n",
            "Epoch 4/20\n",
            "5920/5943 [============================>.] - ETA: 0s - loss: 2.7663 - dense_6_loss: 0.6900 - dense_8_loss: 0.6925 - dense_10_loss: 0.6919 - dense_12_loss: 0.6918 - dense_6_acc: 0.6586 - dense_8_acc: 0.4380 - dense_10_acc: 0.4743 - dense_12_acc: 0.5774\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "5943/5943 [==============================] - 81s 14ms/sample - loss: 2.7656 - dense_6_loss: 0.6903 - dense_8_loss: 0.6917 - dense_10_loss: 0.6916 - dense_12_loss: 0.6916 - dense_6_acc: 0.6586 - dense_8_acc: 0.4383 - dense_10_acc: 0.4740 - dense_12_acc: 0.5777 - val_loss: 2.7742 - val_dense_6_loss: 0.6897 - val_dense_8_loss: 0.6977 - val_dense_10_loss: 0.6963 - val_dense_12_loss: 0.6905 - val_dense_6_acc: 0.6339 - val_dense_8_acc: 0.4671 - val_dense_10_acc: 0.4509 - val_dense_12_acc: 0.5643\n",
            "Epoch 5/20\n",
            "5920/5943 [============================>.] - ETA: 0s - loss: 2.7666 - dense_6_loss: 0.6902 - dense_8_loss: 0.6924 - dense_10_loss: 0.6920 - dense_12_loss: 0.6918 - dense_6_acc: 0.6561 - dense_8_acc: 0.4532 - dense_10_acc: 0.4812 - dense_12_acc: 0.5731\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "5943/5943 [==============================] - 81s 14ms/sample - loss: 2.7658 - dense_6_loss: 0.6902 - dense_8_loss: 0.6916 - dense_10_loss: 0.6917 - dense_12_loss: 0.6919 - dense_6_acc: 0.6567 - dense_8_acc: 0.4530 - dense_10_acc: 0.4806 - dense_12_acc: 0.5733 - val_loss: 2.7744 - val_dense_6_loss: 0.6897 - val_dense_8_loss: 0.6977 - val_dense_10_loss: 0.6961 - val_dense_12_loss: 0.6907 - val_dense_6_acc: 0.6339 - val_dense_8_acc: 0.4690 - val_dense_10_acc: 0.4509 - val_dense_12_acc: 0.5567\n",
            "Epoch 6/20\n",
            "5920/5943 [============================>.] - ETA: 0s - loss: 2.7655 - dense_6_loss: 0.6900 - dense_8_loss: 0.6918 - dense_10_loss: 0.6916 - dense_12_loss: 0.6919 - dense_6_acc: 0.6524 - dense_8_acc: 0.4541 - dense_10_acc: 0.4853 - dense_12_acc: 0.5765Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "5943/5943 [==============================] - 80s 14ms/sample - loss: 2.7658 - dense_6_loss: 0.6907 - dense_8_loss: 0.6918 - dense_10_loss: 0.6916 - dense_12_loss: 0.6917 - dense_6_acc: 0.6524 - dense_8_acc: 0.4538 - dense_10_acc: 0.4853 - dense_12_acc: 0.5768 - val_loss: 2.7743 - val_dense_6_loss: 0.6897 - val_dense_8_loss: 0.6976 - val_dense_10_loss: 0.6961 - val_dense_12_loss: 0.6907 - val_dense_6_acc: 0.6349 - val_dense_8_acc: 0.4681 - val_dense_10_acc: 0.4538 - val_dense_12_acc: 0.5529\n",
            "Epoch 00006: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZg8io6M89Pr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "9e2cb2c9-3e9e-4fc2-99c3-1516f47cee41"
      },
      "source": [
        "y_train_prob=model.predict([X_train,train_images])\n",
        "y_val_prob=model.predict([X_val,val_images])\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,auc\n",
        "'''\n",
        "y_train_pred = [np.argmax(x) for x in y_train_prob[0]]\n",
        "y_val_pred = [np.argmax(x) for x in y_val_prob[0]]\n",
        "print(\"Training Accuracy : {0}%\".format(int(100*accuracy_score(y_train_le, y_train_pred))))\n",
        "print(\"Test Accuracy : {0}%\\n\".format(int(100*accuracy_score(y_val_le, y_val_pred))))\n",
        "print(classification_report(y_val_le, y_val_pred))\n",
        "'''\n",
        "for i in (0,1,2,3):\n",
        "  print(\"Sub-task:\"+str(i))\n",
        "  y_train_pred = [np.argmax(x) for x in y_train_prob[i]]\n",
        "  y_val_pred = [np.argmax(x) for x in y_val_prob[i]]\n",
        "  print(\"Training Accuracy : {0}%\".format(int(100*accuracy_score(y_train[:,i], y_train_pred))))\n",
        "  print(\"Test Accuracy : {0}%\\n\".format(int(100*accuracy_score(y_val[:,i], y_val_pred))))\n",
        "  print(classification_report(y_val[:,i], y_val_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sub-task:0\n",
            "Training Accuracy : 62%\n",
            "Test Accuracy : 60%\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.25      0.23       251\n",
            "           1       0.75      0.71      0.73       798\n",
            "\n",
            "    accuracy                           0.60      1049\n",
            "   macro avg       0.48      0.48      0.48      1049\n",
            "weighted avg       0.62      0.60      0.61      1049\n",
            "\n",
            "Sub-task:1\n",
            "Training Accuracy : 75%\n",
            "Test Accuracy : 74%\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.05      0.08       240\n",
            "           1       0.77      0.95      0.85       809\n",
            "\n",
            "    accuracy                           0.74      1049\n",
            "   macro avg       0.49      0.50      0.47      1049\n",
            "weighted avg       0.64      0.74      0.67      1049\n",
            "\n",
            "Sub-task:2\n",
            "Training Accuracy : 59%\n",
            "Test Accuracy : 56%\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.11      0.18       424\n",
            "           1       0.59      0.88      0.71       625\n",
            "\n",
            "    accuracy                           0.57      1049\n",
            "   macro avg       0.49      0.50      0.44      1049\n",
            "weighted avg       0.51      0.57      0.49      1049\n",
            "\n",
            "Sub-task:3\n",
            "Training Accuracy : 54%\n",
            "Test Accuracy : 50%\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.59      0.61       681\n",
            "           1       0.32      0.36      0.34       368\n",
            "\n",
            "    accuracy                           0.51      1049\n",
            "   macro avg       0.48      0.48      0.47      1049\n",
            "weighted avg       0.52      0.51      0.51      1049\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K1yk43ixGfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_prob=model.predict([X_test,test_images])\n",
        "'''\n",
        "y_test_pred = [np.argmax(x) for x in y_test_prob[0]]\n",
        "with open('A.pkl', 'wb') as f:\n",
        "  pickle.dump(y_test_pred, f)\n",
        "'''\n",
        "for i in (0,1,2,3):\n",
        "  y_test_pred = [np.argmax(x) for x in y_test_prob[i]] #!!!\n",
        "  with open('B'+str(i)+'.pkl', 'wb') as f:\n",
        "    pickle.dump(y_test_pred, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZNMQv26Vy6c",
        "colab_type": "text"
      },
      "source": [
        "##A+C"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAi9arR2EDTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train=[]\n",
        "for i in train_data.index:\n",
        "  labs=[]\n",
        "  x=1\n",
        "  if train_data.at[i,'humour']=='not_funny':\n",
        "    x=0\n",
        "  elif train_data.at[i,'humour']=='very_funny':\n",
        "    x=2\n",
        "  elif train_data.at[i,'humour']=='hilarious':\n",
        "    x=3\n",
        "  labs+=[x]\n",
        "  x=1\n",
        "  if train_data.at[i,'sarcasm']=='not_sarcastic':\n",
        "    x=0\n",
        "  elif train_data.at[i,'sarcasm']=='twisted_meaning':\n",
        "    x=2\n",
        "  elif train_data.at[i,'sarcasm']=='very_twisted':\n",
        "    x=3\n",
        "  labs+=[x]\n",
        "  x=1\n",
        "  if train_data.at[i,'offensive']=='not_offensive':\n",
        "    x=0\n",
        "  elif train_data.at[i,'offensive']=='very_offensive':\n",
        "    x=2\n",
        "  elif train_data.at[i,'offensive']=='hateful_offensive':\n",
        "    x=3\n",
        "  labs+=[x]\n",
        "  y_train.append(labs)\n",
        "\n",
        "y_val=[]\n",
        "for i in val_data.index:\n",
        "  labs=[]\n",
        "  x=1\n",
        "  if val_data.at[i,'humour']=='not_funny':\n",
        "    x=0\n",
        "  elif val_data.at[i,'humour']=='very_funny':\n",
        "    x=2\n",
        "  elif val_data.at[i,'humour']=='hilarious':\n",
        "    x=3\n",
        "  labs+=[x]\n",
        "  x=1\n",
        "  if val_data.at[i,'sarcasm']=='not_sarcastic':\n",
        "    x=0\n",
        "  elif val_data.at[i,'sarcasm']=='twisted_meaning':\n",
        "    x=2\n",
        "  elif val_data.at[i,'sarcasm']=='very_twisted':\n",
        "    x=3\n",
        "  labs+=[x]\n",
        "  x=1\n",
        "  if val_data.at[i,'offensive']=='not_offensive':\n",
        "    x=0\n",
        "  elif val_data.at[i,'offensive']=='very_offensive':\n",
        "    x=2\n",
        "  elif val_data.at[i,'offensive']=='hateful_offensive':\n",
        "    x=3\n",
        "  labs+=[x]\n",
        "  y_val.append(labs)\n",
        "\n",
        "y_train=np.array(y_train)\n",
        "y_val=np.array(y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BsB5bUBV0oc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "921a7f9c-9bd0-462f-d8a8-5e2de43e0af3"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, Activation, ZeroPadding2D\n",
        "from tensorflow.keras.layers import Input,Dense,Bidirectional,Conv2D,MaxPooling2D,Flatten,concatenate,GlobalAveragePooling2D,BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "cw=[]\n",
        "#class_weights = class_weight.compute_class_weight('balanced',np.unique(train_data.Sentiment.values),\n",
        "#                                                  train_data.Sentiment.values)\n",
        "#class_weights= {0:class_weights[0],1:class_weights[1],2:class_weights[2]}\n",
        "#cw.append(class_weights)\n",
        "for cls in (0,1,2):\n",
        "  class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train[:,cls]), y_train[:,cls])\n",
        "  class_weights= {0:class_weights[0],1:class_weights[1],2:class_weights[2],3:class_weights[3]}\n",
        "  cw.append(class_weights)\n",
        "\n",
        "vocabulary_size = len(tk.word_counts.keys())+1\n",
        "max_words = 64\n",
        "embedding_size = 32\n",
        "dp=0\n",
        "\n",
        "txt_input = Input(shape=(64,))\n",
        "x = Embedding(vocabulary_size, embedding_size, input_length=max_words,)(txt_input)\n",
        "x = Dropout(dp)(x)\n",
        "x = Bidirectional(LSTM(32, recurrent_dropout=dp, kernel_regularizer=l2(0.01), return_sequences=True))(x)\n",
        "x = Dropout(dp)(x)\n",
        "x = Bidirectional(LSTM(32, recurrent_dropout=dp, kernel_regularizer=l2(0.01)))(x)\n",
        "x = Dropout(dp)(x)\n",
        "x = Dense(128, activation=\"relu\")(x)\n",
        "x = Model(inputs=txt_input, outputs=x)\n",
        "\n",
        "img_input = Input(shape=(224,224,3))\n",
        "y = Conv2D(96, (11, 11), padding='same')(img_input)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = Conv2D(128, (5, 5), padding='same')(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = ZeroPadding2D((1, 1))(y)\n",
        "y = Conv2D(128, (3, 3), padding='same')(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = ZeroPadding2D((1, 1))(y)\n",
        "y = Conv2D(256, (3, 3), padding='same')(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = ZeroPadding2D((1, 1))(y)\n",
        "y = Conv2D(256, (3, 3), padding='same')(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "y = Dropout(dp)(y)\n",
        "\n",
        "y = Flatten()(y)\n",
        "y = Dense(512)(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "y = Dropout(dp)(y)\n",
        "y = Dense(128)(y)\n",
        "y = BatchNormalization()(y)\n",
        "y = Activation('relu')(y)\n",
        "\n",
        "y = Model(inputs=img_input, outputs=y)\n",
        "\n",
        "shared_layer = Dense(256, activation=\"relu\")\n",
        "x_sh = shared_layer(x.output)\n",
        "y_sh = shared_layer(y.output)\n",
        "\n",
        "combined = concatenate([x_sh, y_sh])#x.output, y.output])\n",
        "z = Dense(256, activation=\"relu\")(combined)\n",
        "\n",
        "#task_a = Dense(3, activation=\"softmax\")(z)\n",
        "task_c0 = Dense(128, activation=\"relu\")(z)\n",
        "task_c0 = Dense(4, activation=\"softmax\")(task_c0)\n",
        "task_c1 = Dense(128, activation=\"relu\")(z)\n",
        "task_c1 = Dense(4, activation=\"softmax\")(task_c1)\n",
        "task_c2 = Dense(128, activation=\"relu\")(z)\n",
        "task_c2 = Dense(4, activation=\"softmax\")(task_c2)\n",
        "\n",
        "model = Model(inputs=[x.input, y.input], outputs=[task_c0,task_c1,task_c2])\n",
        "#adam=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, decay=1e-3)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['acc'], loss_weights=[1,1,1])\n",
        "#model.summary()\n",
        "early=EarlyStopping(patience=5, restore_best_weights=False, verbose=1)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, verbose=1)\n",
        "\n",
        "history=model.fit([X_train, train_images], [y_train[:,0],y_train[:,1],y_train[:,2]],\n",
        "\tvalidation_data=([X_val, val_images], [y_val[:,0],y_val[:,1],y_val[:,2]]),\n",
        "\tepochs=20, batch_size=32, class_weight=cw, callbacks=[early,reduce_lr])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5943 samples, validate on 1049 samples\n",
            "Epoch 1/20\n",
            "5943/5943 [==============================] - 87s 15ms/sample - loss: 4.9748 - dense_30_loss: 1.3971 - dense_32_loss: 1.3964 - dense_34_loss: 1.4046 - dense_30_acc: 0.2315 - dense_32_acc: 0.2495 - dense_34_acc: 0.2305 - val_loss: 4.2021 - val_dense_30_loss: 1.3813 - val_dense_32_loss: 1.3842 - val_dense_34_loss: 1.3770 - val_dense_30_acc: 0.2669 - val_dense_32_acc: 0.2269 - val_dense_34_acc: 0.3699\n",
            "Epoch 2/20\n",
            "5943/5943 [==============================] - 82s 14ms/sample - loss: 4.1804 - dense_30_loss: 1.3887 - dense_32_loss: 1.3888 - dense_34_loss: 1.3924 - dense_30_acc: 0.2558 - dense_32_acc: 0.2717 - dense_34_acc: 0.2463 - val_loss: 4.1814 - val_dense_30_loss: 1.4138 - val_dense_32_loss: 1.3948 - val_dense_34_loss: 1.3730 - val_dense_30_acc: 0.1678 - val_dense_32_acc: 0.1735 - val_dense_34_acc: 0.3203\n",
            "Epoch 3/20\n",
            "5920/5943 [============================>.] - ETA: 0s - loss: 4.1642 - dense_30_loss: 1.3867 - dense_32_loss: 1.3873 - dense_34_loss: 1.3901 - dense_30_acc: 0.2431 - dense_32_acc: 0.2091 - dense_34_acc: 0.2164\n",
            "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "5943/5943 [==============================] - 83s 14ms/sample - loss: 4.1622 - dense_30_loss: 1.3858 - dense_32_loss: 1.3871 - dense_34_loss: 1.3886 - dense_30_acc: 0.2431 - dense_32_acc: 0.2093 - dense_34_acc: 0.2157 - val_loss: 4.1908 - val_dense_30_loss: 1.4002 - val_dense_32_loss: 1.3814 - val_dense_34_loss: 1.4091 - val_dense_30_acc: 0.1916 - val_dense_32_acc: 0.2402 - val_dense_34_acc: 0.0534\n",
            "Epoch 4/20\n",
            "5920/5943 [============================>.] - ETA: 0s - loss: 4.1386 - dense_30_loss: 1.3819 - dense_32_loss: 1.3792 - dense_34_loss: 1.3774 - dense_30_acc: 0.1894 - dense_32_acc: 0.2579 - dense_34_acc: 0.1277\n",
            "Epoch 00004: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "5943/5943 [==============================] - 82s 14ms/sample - loss: 4.1401 - dense_30_loss: 1.3815 - dense_32_loss: 1.3793 - dense_34_loss: 1.3800 - dense_30_acc: 0.1890 - dense_32_acc: 0.2576 - dense_34_acc: 0.1279 - val_loss: 4.1844 - val_dense_30_loss: 1.3940 - val_dense_32_loss: 1.3836 - val_dense_34_loss: 1.4070 - val_dense_30_acc: 0.2126 - val_dense_32_acc: 0.2431 - val_dense_34_acc: 0.1030\n",
            "Epoch 5/20\n",
            "5920/5943 [============================>.] - ETA: 0s - loss: 4.1230 - dense_30_loss: 1.3773 - dense_32_loss: 1.3735 - dense_34_loss: 1.3722 - dense_30_acc: 0.2240 - dense_32_acc: 0.2542 - dense_34_acc: 0.1459\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "5943/5943 [==============================] - 81s 14ms/sample - loss: 4.1235 - dense_30_loss: 1.3771 - dense_32_loss: 1.3736 - dense_34_loss: 1.3729 - dense_30_acc: 0.2238 - dense_32_acc: 0.2541 - dense_34_acc: 0.1461 - val_loss: 4.1842 - val_dense_30_loss: 1.3952 - val_dense_32_loss: 1.3829 - val_dense_34_loss: 1.4060 - val_dense_30_acc: 0.2107 - val_dense_32_acc: 0.2412 - val_dense_34_acc: 0.1163\n",
            "Epoch 6/20\n",
            "5920/5943 [============================>.] - ETA: 0s - loss: 4.1247 - dense_30_loss: 1.3778 - dense_32_loss: 1.3728 - dense_34_loss: 1.3741 - dense_30_acc: 0.2223 - dense_32_acc: 0.2667 - dense_34_acc: 0.1459\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "5943/5943 [==============================] - 81s 14ms/sample - loss: 4.1227 - dense_30_loss: 1.3774 - dense_32_loss: 1.3719 - dense_34_loss: 1.3726 - dense_30_acc: 0.2224 - dense_32_acc: 0.2667 - dense_34_acc: 0.1461 - val_loss: 4.1845 - val_dense_30_loss: 1.3956 - val_dense_32_loss: 1.3827 - val_dense_34_loss: 1.4061 - val_dense_30_acc: 0.2135 - val_dense_32_acc: 0.2469 - val_dense_34_acc: 0.1182\n",
            "Epoch 7/20\n",
            "5920/5943 [============================>.] - ETA: 0s - loss: 4.1248 - dense_30_loss: 1.3772 - dense_32_loss: 1.3748 - dense_34_loss: 1.3728 - dense_30_acc: 0.2275 - dense_32_acc: 0.2608 - dense_34_acc: 0.1454\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
            "5943/5943 [==============================] - 81s 14ms/sample - loss: 4.1234 - dense_30_loss: 1.3773 - dense_32_loss: 1.3727 - dense_34_loss: 1.3728 - dense_30_acc: 0.2273 - dense_32_acc: 0.2608 - dense_34_acc: 0.1450 - val_loss: 4.1844 - val_dense_30_loss: 1.3962 - val_dense_32_loss: 1.3824 - val_dense_34_loss: 1.4058 - val_dense_30_acc: 0.2088 - val_dense_32_acc: 0.2469 - val_dense_34_acc: 0.1153\n",
            "Epoch 00007: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUynURHlpEO7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "8aa181c3-7cb2-436d-c189-fba790bcc2de"
      },
      "source": [
        "y_train_prob=model.predict([X_train,train_images])\n",
        "y_val_prob=model.predict([X_val,val_images])\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,auc\n",
        "'''\n",
        "y_train_pred = [np.argmax(x) for x in y_train_prob[0]]\n",
        "y_val_pred = [np.argmax(x) for x in y_val_prob[0]]\n",
        "print(\"Training Accuracy : {0}%\".format(int(100*accuracy_score(y_train_le, y_train_pred))))\n",
        "print(\"Test Accuracy : {0}%\\n\".format(int(100*accuracy_score(y_val_le, y_val_pred))))\n",
        "print(classification_report(y_val_le, y_val_pred))\n",
        "'''\n",
        "for i in (0,1,2):\n",
        "  print(\"Sub-task:\"+str(i))\n",
        "  y_train_pred = [np.argmax(x) for x in y_train_prob[i]]\n",
        "  y_val_pred = [np.argmax(x) for x in y_val_prob[i]]\n",
        "  print(\"Training Accuracy : {0}%\".format(int(100*accuracy_score(y_train[:,i], y_train_pred))))\n",
        "  print(\"Test Accuracy : {0}%\\n\".format(int(100*accuracy_score(y_val[:,i], y_val_pred))))\n",
        "  print(classification_report(y_val[:,i], y_val_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sub-task:0\n",
            "Training Accuracy : 22%\n",
            "Test Accuracy : 20%\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.14      0.16       251\n",
            "           1       0.32      0.17      0.22       392\n",
            "           2       0.29      0.18      0.22       286\n",
            "           3       0.14      0.57      0.22       120\n",
            "\n",
            "    accuracy                           0.21      1049\n",
            "   macro avg       0.24      0.26      0.21      1049\n",
            "weighted avg       0.26      0.21      0.21      1049\n",
            "\n",
            "Sub-task:1\n",
            "Training Accuracy : 26%\n",
            "Test Accuracy : 24%\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.24      0.33      0.28       240\n",
            "           1       0.52      0.27      0.35       524\n",
            "           2       0.20      0.07      0.11       224\n",
            "           3       0.06      0.39      0.11        61\n",
            "\n",
            "    accuracy                           0.25      1049\n",
            "   macro avg       0.26      0.27      0.21      1049\n",
            "weighted avg       0.36      0.25      0.27      1049\n",
            "\n",
            "Sub-task:2\n",
            "Training Accuracy : 14%\n",
            "Test Accuracy : 11%\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.13      0.19       424\n",
            "           1       0.39      0.04      0.07       389\n",
            "           2       0.17      0.15      0.16       207\n",
            "           3       0.03      0.69      0.06        29\n",
            "\n",
            "    accuracy                           0.12      1049\n",
            "   macro avg       0.24      0.25      0.12      1049\n",
            "weighted avg       0.33      0.12      0.14      1049\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdTBPVRnZ8Hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_prob=model.predict([X_test,test_images])\n",
        "'''\n",
        "y_test_pred = [np.argmax(x) for x in y_test_prob[0]]\n",
        "with open('A.pkl', 'wb') as f:\n",
        "  pickle.dump(y_test_pred, f)\n",
        "'''\n",
        "for i in (0,1,2):\n",
        "  y_test_pred = [np.argmax(x) for x in y_test_prob[i]] #!!!\n",
        "  with open('C'+str(i)+'.pkl', 'wb') as f:\n",
        "    pickle.dump(y_test_pred, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFD89eynEwQH",
        "colab_type": "text"
      },
      "source": [
        "#Test Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwFmF3G9MKdw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "with open('A.pkl', 'rb') as A, open('B0.pkl', 'rb') as B0, open('B1.pkl', 'rb') as B1, open('B2.pkl', 'rb') as B2, open('B3.pkl', 'rb') as B3, open('C0.pkl', 'rb') as C0, open('C1.pkl', 'rb') as C1, open('C2.pkl', 'rb') as C2:\n",
        "  a = pickle.load(A)\n",
        "  b0 = pickle.load(B0)\n",
        "  b1 = pickle.load(B1)\n",
        "  b2 = pickle.load(B2)\n",
        "  b3 = pickle.load(B3)\n",
        "  c0 = pickle.load(C0)\n",
        "  c1 = pickle.load(C1)\n",
        "  c2 = pickle.load(C2)\n",
        "\n",
        "#w = (np.max(y_test_prob[:,1])-0.5)/3\n",
        "#c3=[0 if i<0.5 else int((i-0.5)/w)+1 for i in y_test_prob[:,1]]\n",
        "c3=b3\n",
        "\n",
        "out=[]\n",
        "for i in range(1878):\n",
        "  out.append(str(a[i]-1)+'_'+str(b0[i])+str(b1[i])+str(b2[i])+str(b3[i])+'_'+str(c0[i])+str(c1[i])+str(c2[i])+str(c3[i])+'\\n')\n",
        "\n",
        "with open('answer.txt', 'w') as f:\n",
        "  f.writelines(out)\n",
        "\n",
        "#!zip res.zip answer.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj0bQB5XE6XV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "72f372ff-d0a3-406d-cda5-3a84ea9deb9e"
      },
      "source": [
        "y_test_prob"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5188661 , 0.48113385],\n",
              "       [0.5129394 , 0.4870606 ],\n",
              "       [0.4712976 , 0.5287024 ],\n",
              "       ...,\n",
              "       [0.49387637, 0.50612366],\n",
              "       [0.43651247, 0.5634875 ],\n",
              "       [0.40774596, 0.59225404]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t2jHpkyfslT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f20027f-62a8-42c5-c2a7-5c75ece48b61"
      },
      "source": [
        "np.min(y_test_prob[:,1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.35645983"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    }
  ]
}